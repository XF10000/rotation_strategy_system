"""
æ•°æ®è·å–å™¨æ¨¡å—
æ”¯æŒä»ä¸åŒæ•°æ®æºè·å–è‚¡ç¥¨æ•°æ®
"""

import logging
from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from typing import Dict, List

import akshare as ak
import pandas as pd

try:
    import tushare as ts
    TUSHARE_AVAILABLE = True
except ImportError:
    TUSHARE_AVAILABLE = False
    ts = None

from .exceptions import DataFetchError

logger = logging.getLogger(__name__)

class DataFetcher(ABC):
    """æ•°æ®è·å–å™¨åŸºç±»"""
    
    @abstractmethod
    def get_stock_data(self, code: str, start_date: str, end_date: str = None, 
                      period: str = 'weekly') -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨å†å²æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç  (å¦‚ '601088')
            start_date: å¼€å§‹æ—¥æœŸ ('YYYY-MM-DD')
            end_date: ç»“æŸæ—¥æœŸ ('YYYY-MM-DD', Noneè¡¨ç¤ºå½“å‰æ—¥æœŸ)
            period: æ•°æ®å‘¨æœŸ ('daily', 'weekly', 'monthly')
            
        Returns:
            pd.DataFrame: æ ‡å‡†åŒ–çš„è‚¡ç¥¨æ•°æ®
            
        Raises:
            DataFetchError: æ•°æ®è·å–å¤±è´¥
        """
    
    def get_multiple_stocks_data(self, codes: List[str], start_date: str, 
                               end_date: str = None, period: str = 'weekly') -> Dict[str, pd.DataFrame]:
        """
        æ‰¹é‡è·å–å¤šåªè‚¡ç¥¨æ•°æ®
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            period: æ•°æ®å‘¨æœŸ
            
        Returns:
            Dict[str, pd.DataFrame]: è‚¡ç¥¨ä»£ç åˆ°æ•°æ®çš„æ˜ å°„
        """
        result = {}
        failed_codes = []
        
        for code in codes:
            try:
                logger.info(f"è·å–è‚¡ç¥¨ {code} çš„æ•°æ®...")
                data = self.get_stock_data(code, start_date, end_date, period)
                result[code] = data
                logger.info(f"æˆåŠŸè·å–è‚¡ç¥¨ {code} çš„æ•°æ®ï¼Œå…± {len(data)} æ¡è®°å½•")
            except Exception as e:
                logger.error(f"è·å–è‚¡ç¥¨ {code} æ•°æ®å¤±è´¥: {str(e)}")
                failed_codes.append(code)
        
        if failed_codes:
            logger.warning(f"ä»¥ä¸‹è‚¡ç¥¨æ•°æ®è·å–å¤±è´¥: {failed_codes}")
        
        return result

class AkshareDataFetcher(DataFetcher):
    """Akshareæ•°æ®è·å–å™¨å®ç°"""
    
    def __init__(self):
        """åˆå§‹åŒ–Akshareæ•°æ®è·å–å™¨"""
        self.source_name = "akshare"
        self.last_request_time = None  # è®°å½•ä¸Šæ¬¡è¯·æ±‚æ—¶é—´
        self.min_request_interval = 3.0  # æœ€å°è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰- å¢åŠ åˆ°3ç§’ä»¥é¿å…è¿æ¥ä¸­æ–­
        logger.info("åˆå§‹åŒ–Akshareæ•°æ®è·å–å™¨")
    
    def get_stock_data(self, code: str, start_date: str, end_date: str = None, 
                      period: str = 'weekly') -> pd.DataFrame:
        """
        ä»Akshareè·å–è‚¡ç¥¨å†å²æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç  (å¦‚ '601088')
            start_date: å¼€å§‹æ—¥æœŸ ('YYYY-MM-DD')
            end_date: ç»“æŸæ—¥æœŸ ('YYYY-MM-DD', Noneè¡¨ç¤ºå½“å‰æ—¥æœŸ)
            period: æ•°æ®å‘¨æœŸ ('daily', 'weekly', 'monthly')
            
        Returns:
            pd.DataFrame: æ ‡å‡†åŒ–çš„è‚¡ç¥¨æ•°æ®
            
        Raises:
            DataFetchError: æ•°æ®è·å–å¤±è´¥
        """
        try:
            # å‚æ•°éªŒè¯
            if not self._validate_stock_code(code):
                raise DataFetchError(f"æ— æ•ˆçš„è‚¡ç¥¨ä»£ç : {code}")
            
            if not self._validate_date_format(start_date):
                raise DataFetchError(f"æ— æ•ˆçš„å¼€å§‹æ—¥æœŸæ ¼å¼: {start_date}")
            
            if end_date and not self._validate_date_format(end_date):
                raise DataFetchError(f"æ— æ•ˆçš„ç»“æŸæ—¥æœŸæ ¼å¼: {end_date}")
            
            # è®¾ç½®é»˜è®¤ç»“æŸæ—¥æœŸ
            if end_date is None:
                end_date = datetime.now().strftime('%Y%m%d')
            else:
                # æ­£ç¡®å¤„ç†æ—¥æœŸæ ¼å¼ï¼Œæ”¯æŒç¼ºå°‘å‰å¯¼é›¶çš„æƒ…å†µ
                try:
                    parsed_end_date = datetime.strptime(end_date, '%Y-%m-%d')
                    end_date = parsed_end_date.strftime('%Y%m%d')
                except ValueError:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•å¤„ç†ç¼ºå°‘å‰å¯¼é›¶çš„æƒ…å†µ
                    parts = end_date.split('-')
                    if len(parts) == 3:
                        year, month, day = parts
                        month = month.zfill(2)
                        day = day.zfill(2)
                        fixed_date = f'{year}-{month}-{day}'
                        parsed_end_date = datetime.strptime(fixed_date, '%Y-%m-%d')
                        end_date = parsed_end_date.strftime('%Y%m%d')
                    else:
                        raise DataFetchError(f"æ— æ³•è§£æç»“æŸæ—¥æœŸæ ¼å¼: {end_date}")
            
            # æ­£ç¡®å¤„ç†å¼€å§‹æ—¥æœŸæ ¼å¼
            try:
                parsed_start_date = datetime.strptime(start_date, '%Y-%m-%d')
                start_date = parsed_start_date.strftime('%Y%m%d')
            except ValueError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•å¤„ç†ç¼ºå°‘å‰å¯¼é›¶çš„æƒ…å†µ
                parts = start_date.split('-')
                if len(parts) == 3:
                    year, month, day = parts
                    month = month.zfill(2)
                    day = day.zfill(2)
                    fixed_date = f'{year}-{month}-{day}'
                    parsed_start_date = datetime.strptime(fixed_date, '%Y-%m-%d')
                    start_date = parsed_start_date.strftime('%Y%m%d')
                else:
                    raise DataFetchError(f"æ— æ³•è§£æå¼€å§‹æ—¥æœŸæ ¼å¼: {start_date}")
            
            # æ˜ å°„å‘¨æœŸå‚æ•°
            period_map = {
                'daily': 'daily',
                'weekly': 'weekly', 
                'monthly': 'monthly'
            }
            
            if period not in period_map:
                raise DataFetchError(f"ä¸æ”¯æŒçš„æ•°æ®å‘¨æœŸ: {period}")
            
            ak_period = period_map[period]
            
            logger.debug(f"ä»Akshareè·å–æ•°æ®: {code}, {start_date}-{end_date}, {ak_period}")
            
            # è°ƒç”¨akshareæ¥å£è·å–æ•°æ®ï¼Œå¢åŠ é‡è¯•æœºåˆ¶
            max_retries = 5  # å¢åŠ é‡è¯•æ¬¡æ•°
            df = None
            
            for attempt in range(max_retries):
                try:
                    import time

                    # æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…è§¦å‘åçˆ¬è™«
                    if self.last_request_time is not None:
                        elapsed = time.time() - self.last_request_time
                        if elapsed < self.min_request_interval:
                            sleep_time = self.min_request_interval - elapsed
                            logger.debug(f"è¯·æ±‚é—´éš”æ§åˆ¶ï¼šç­‰å¾… {sleep_time:.2f} ç§’")
                            time.sleep(sleep_time)
                    
                    # é‡è¯•æ—¶å¢åŠ é¢å¤–å»¶è¿Ÿ
                    if attempt > 0:
                        wait_time = 5 + attempt * 5  # å¢åŠ é‡è¯•ç­‰å¾…æ—¶é—´: 5, 10, 15, 20...
                        logger.debug(f"é‡è¯•ç­‰å¾…: {wait_time}ç§’")
                        time.sleep(wait_time)
                    
                    # æ›´æ–°è¯·æ±‚æ—¶é—´
                    self.last_request_time = time.time()
                    
                    logger.debug(f"å°è¯•è·å–è‚¡ç¥¨ {code} æ•°æ®ï¼Œç¬¬ {attempt + 1} æ¬¡")
                    
                    # å°è¯•ä¸åŒçš„è·å–æ–¹å¼
                    if attempt < 3:
                        # å‰3æ¬¡ä½¿ç”¨æ ‡å‡†æ–¹å¼
                        df = ak.stock_zh_a_hist(
                            symbol=code,
                            period=ak_period,
                            start_date=start_date,
                            end_date=end_date,
                            adjust=""  # ä¸å¤æƒæ•°æ®
                        )
                    else:
                        # åç»­å°è¯•ä½¿ç”¨ä¸åŒå‚æ•°
                        df = ak.stock_zh_a_hist(
                            symbol=code,
                            period="daily",  # æ”¹ç”¨æ—¥çº¿æ•°æ®
                            start_date=start_date,
                            end_date=end_date,
                            adjust=""  # ä¸å¤æƒæ•°æ®
                        )
                    
                    if df is not None and not df.empty:
                        logger.debug(f"æˆåŠŸè·å–è‚¡ç¥¨ {code} æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
                        break
                    else:
                        logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•è·å–è‚¡ç¥¨ {code} æ•°æ®ä¸ºç©º")
                        
                except Exception as e:
                    logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•è·å–è‚¡ç¥¨ {code} æ•°æ®å¤±è´¥: {str(e)}")
                    # æ£€æµ‹æ˜¯å¦ä¸ºè¿æ¥ä¸­æ–­é”™è¯¯
                    is_connection_error = "RemoteDisconnected" in str(e) or "Connection aborted" in str(e)
                    
                    if attempt < max_retries - 1:
                        # å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                        base_wait = 10 if is_connection_error else 3
                        sleep_time = base_wait + attempt * 5
                        logger.warning(f"ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                        time.sleep(sleep_time)
                    else:
                        # æœ€åä¸€æ¬¡å°è¯•ï¼Œè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                        logger.error(f"æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè‚¡ç¥¨ {code} å¯èƒ½æš‚æ—¶æ— æ³•è·å–æ•°æ®")
            
            if df is None or df.empty:
                raise DataFetchError(f"æœªè·å–åˆ°è‚¡ç¥¨ {code} çš„æ•°æ®")
            
            # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
            df = self._standardize_data_format(df)
            
            logger.debug(f"æˆåŠŸè·å–è‚¡ç¥¨ {code} æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
            return df
            
        except Exception as e:
            error_msg = f"è·å–è‚¡ç¥¨ {code} æ•°æ®å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise DataFetchError(error_msg) from e
    
    def _standardize_data_format(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
        
        Args:
            df: åŸå§‹akshareæ•°æ®
            
        Returns:
            pd.DataFrame: æ ‡å‡†åŒ–åçš„æ•°æ®
        """
        try:
            # é‡å‘½ååˆ—åä¸ºè‹±æ–‡æ ‡å‡†æ ¼å¼
            column_mapping = {
                'æ—¥æœŸ': 'date',
                'å¼€ç›˜': 'open',
                'æ”¶ç›˜': 'close', 
                'æœ€é«˜': 'high',
                'æœ€ä½': 'low',
                'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'amount',
                'æŒ¯å¹…': 'amplitude',
                'æ¶¨è·Œå¹…': 'pct_change',
                'æ¶¨è·Œé¢': 'change',
                'æ¢æ‰‹ç‡': 'turnover_rate'
            }
            
            # é‡å‘½åå­˜åœ¨çš„åˆ—
            existing_columns = {k: v for k, v in column_mapping.items() if k in df.columns}
            df = df.rename(columns=existing_columns)
            
            # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
            required_columns = ['date', 'open', 'high', 'low', 'close', 'volume']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                raise DataFetchError(f"ç¼ºå°‘å¿…è¦çš„æ•°æ®åˆ—: {missing_columns}")
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            df['date'] = pd.to_datetime(df['date'])
            
            # è®¾ç½®æ—¥æœŸä¸ºç´¢å¼•
            df = df.set_index('date')
            
            # ç¡®ä¿æ•°å€¼åˆ—ä¸ºfloatç±»å‹
            numeric_columns = ['open', 'high', 'low', 'close', 'volume']
            if 'amount' in df.columns:
                numeric_columns.append('amount')
            if 'turnover_rate' in df.columns:
                numeric_columns.append('turnover_rate')
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_index()
            
            # é€‰æ‹©éœ€è¦çš„åˆ—
            output_columns = ['open', 'high', 'low', 'close', 'volume']
            if 'amount' in df.columns:
                output_columns.append('amount')
            if 'turnover_rate' in df.columns:
                output_columns.append('turnover_rate')
            
            df = df[output_columns]
            
            return df
            
        except Exception as e:
            raise DataFetchError(f"æ•°æ®æ ¼å¼æ ‡å‡†åŒ–å¤±è´¥: {str(e)}") from e
    
    def _validate_stock_code(self, code: str) -> bool:
        """
        éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            bool: æ˜¯å¦æœ‰æ•ˆ
        """
        if not code or not isinstance(code, str):
            return False
        
        # ç®€å•éªŒè¯ï¼š6ä½æ•°å­—
        return len(code) == 6 and code.isdigit()
    
    def _validate_date_format(self, date_str: str) -> bool:
        """
        éªŒè¯æ—¥æœŸæ ¼å¼
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            
        Returns:
            bool: æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            datetime.strptime(date_str, '%Y-%m-%d')
            return True
        except ValueError:
            return False
    
    def get_latest_trading_date(self) -> str:
        """
        è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸ
        
        Returns:
            str: æœ€æ–°äº¤æ˜“æ—¥æœŸ ('YYYY-MM-DD')
        """
        try:
            # è·å–æœ€è¿‘çš„äº¤æ˜“æ—¥å†
            today = datetime.now()
            
            # ç®€å•å®ç°ï¼šå¦‚æœæ˜¯å‘¨æœ«ï¼Œå›é€€åˆ°å‘¨äº”
            weekday = today.weekday()
            if weekday == 5:  # å‘¨å…­
                latest_date = today - timedelta(days=1)
            elif weekday == 6:  # å‘¨æ—¥
                latest_date = today - timedelta(days=2)
            else:
                latest_date = today
            
            return latest_date.strftime('%Y-%m-%d')
            
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸå¤±è´¥: {str(e)}")
            return datetime.now().strftime('%Y-%m-%d')
    
    def test_connection(self) -> bool:
        """
        æµ‹è¯•æ•°æ®æºè¿æ¥
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æ­£å¸¸
        """
        try:
            # å°è¯•è·å–ä¸€åªè‚¡ç¥¨çš„å°‘é‡æ•°æ®æ¥æµ‹è¯•è¿æ¥
            test_code = "000001"  # å¹³å®‰é“¶è¡Œ
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=7)).strftime('%Y%m%d')
            
            df = ak.stock_zh_a_hist(
                symbol=test_code,
                period="daily",
                start_date=start_date,
                end_date=end_date,
                adjust="qfq"
            )
            
            return df is not None and not df.empty
            
        except Exception as e:
            logger.error(f"æµ‹è¯•Akshareè¿æ¥å¤±è´¥: {str(e)}")
            return False
    
    def get_dividend_data(self, code: str, start_date: str, end_date: str = None, 
                         use_cache: bool = True) -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨åˆ†çº¢é…è‚¡æ•°æ®ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
        
        Args:
            code: è‚¡ç¥¨ä»£ç  (å¦‚ '601088')
            start_date: å¼€å§‹æ—¥æœŸ ('YYYY-MM-DD')
            end_date: ç»“æŸæ—¥æœŸ ('YYYY-MM-DD', Noneè¡¨ç¤ºå½“å‰æ—¥æœŸ)
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            pd.DataFrame: åˆ†çº¢é…è‚¡æ•°æ®
            
        Raises:
            DataFetchError: æ•°æ®è·å–å¤±è´¥
        """
        try:
            # å¯¼å…¥æ•°æ®å­˜å‚¨æ¨¡å—
            from .data_storage import DataStorage
            storage = DataStorage()
            
            # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦çš„æ—¥æœŸèŒƒå›´å·²è¢«ç¼“å­˜è¦†ç›–
            if use_cache:
                # æ£€æŸ¥æŒ‡å®šæ—¥æœŸèŒƒå›´æ˜¯å¦å·²è¢«ç¼“å­˜å®Œå…¨è¦†ç›–
                if storage.is_dividend_date_range_cached(code, start_date or '1990-01-01', 
                                                       end_date or datetime.now().strftime('%Y-%m-%d')):
                    cached_data = storage.load_dividend_data(code)
                    if cached_data is not None:
                        logger.info(f"ğŸ“¦ ä½¿ç”¨åˆ†çº¢é…è‚¡ç¼“å­˜æ•°æ®: {code}")
                        
                        # æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤ç¼“å­˜æ•°æ®
                        filtered_data = cached_data.copy()
                        if start_date:
                            start_dt = pd.to_datetime(start_date)
                            filtered_data = filtered_data[filtered_data.index >= start_dt]
                        
                        if end_date:
                            end_dt = pd.to_datetime(end_date)
                            filtered_data = filtered_data[filtered_data.index <= end_dt]
                        
                        logger.info(f"âœ… ç¼“å­˜åˆ†çº¢é…è‚¡æ•°æ®è¿‡æ»¤å: {code}, {len(filtered_data)} æ¡è®°å½•")
                        return filtered_data
                else:
                    # ç¼“å­˜èŒƒå›´ä¸è¶³ï¼Œéœ€è¦ä»ç½‘ç»œè·å–
                    cache_coverage = storage.get_dividend_cache_coverage(code)
                    if cache_coverage:
                        logger.info(f"ğŸ“Š {code} ç¼“å­˜èŒƒå›´ä¸è¶³ï¼Œéœ€è¦ç½‘ç»œè·å–")
                        logger.info(f"  ç¼“å­˜èŒƒå›´: {cache_coverage['start_date']} åˆ° {cache_coverage['end_date']}")
                        logger.info(f"  éœ€è¦èŒƒå›´: {start_date or '1990-01-01'} åˆ° {end_date or datetime.now().strftime('%Y-%m-%d')}")
                    else:
                        logger.info(f"ğŸ“Š {code} æ— åˆ†çº¢é…è‚¡ç¼“å­˜ï¼Œéœ€è¦ç½‘ç»œè·å–")
            
            # ç¼“å­˜ä¸å­˜åœ¨æˆ–è¿‡æœŸï¼Œä»ç½‘ç»œè·å–
            logger.info(f"ğŸŒ ä»ç½‘ç»œè·å–è‚¡ç¥¨ {code} çš„åˆ†çº¢é…è‚¡æ•°æ®...")
            
            # æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…è§¦å‘åçˆ¬è™«
            import time
            if self.last_request_time is not None:
                elapsed = time.time() - self.last_request_time
                if elapsed < self.min_request_interval:
                    sleep_time = self.min_request_interval - elapsed
                    logger.debug(f"è¯·æ±‚é—´éš”æ§åˆ¶ï¼šç­‰å¾… {sleep_time:.2f} ç§’")
                    time.sleep(sleep_time)
            
            # æ›´æ–°è¯·æ±‚æ—¶é—´
            self.last_request_time = time.time()
            
            # ä½¿ç”¨å¯ç”¨çš„akshare API
            dividend_data = ak.stock_history_dividend_detail(symbol=code)
            
            if dividend_data is None or dividend_data.empty:
                logger.warning(f"æœªè·å–åˆ°è‚¡ç¥¨ {code} çš„åˆ†çº¢é…è‚¡æ•°æ®")
                # å³ä½¿æ˜¯ç©ºæ•°æ®ä¹Ÿè¦ç¼“å­˜ï¼Œé¿å…é‡å¤è¯·æ±‚
                if use_cache:
                    empty_df = pd.DataFrame()
                    storage.save_dividend_data(empty_df, code)
                return pd.DataFrame()
            
            logger.info(f"åŸå§‹åˆ†çº¢æ•°æ®åˆ—å: {list(dividend_data.columns)}")
            logger.info(f"åŸå§‹æ•°æ®æ ·ä¾‹:\n{dividend_data.head(2)}")
            
            # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
            processed_data = self._process_dividend_data(dividend_data)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if use_cache and not processed_data.empty:
                storage.save_dividend_data(processed_data, code)
                logger.info(f"ğŸ’¾ åˆ†çº¢é…è‚¡æ•°æ®å·²ç¼“å­˜: {code}")
            
            # æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤
            filtered_data = processed_data.copy()
            if start_date:
                start_dt = pd.to_datetime(start_date)
                filtered_data = filtered_data[filtered_data.index >= start_dt]
            
            if end_date:
                end_dt = pd.to_datetime(end_date)
                filtered_data = filtered_data[filtered_data.index <= end_dt]
            
            logger.info(f"æˆåŠŸè·å–è‚¡ç¥¨ {code} çš„åˆ†çº¢é…è‚¡æ•°æ®ï¼Œå…± {len(filtered_data)} æ¡è®°å½•")
            return filtered_data
            
        except Exception as e:
            error_msg = f"è·å–è‚¡ç¥¨ {code} åˆ†çº¢é…è‚¡æ•°æ®å¤±è´¥: {str(e)}"
            logger.warning(error_msg)
            # è¿”å›ç©ºæ•°æ®è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥å…å½±å“æ•´ä¸ªå›æµ‹
            return pd.DataFrame()
    
    def align_dividend_with_weekly_data(self, weekly_data: pd.DataFrame, 
                                      dividend_data: pd.DataFrame) -> pd.DataFrame:
        """
        å°†åˆ†çº¢é…è‚¡æ•°æ®ä¸å‘¨çº¿æ•°æ®å¯¹é½
        
        Args:
            weekly_data: å‘¨çº¿æ•°æ®
            dividend_data: åˆ†çº¢é…è‚¡æ•°æ®
            
        Returns:
            pd.DataFrame: å¯¹é½åçš„å‘¨çº¿æ•°æ®ï¼ŒåŒ…å«åˆ†çº¢é…è‚¡ä¿¡æ¯
        """
        try:
            if dividend_data.empty:
                # å¦‚æœæ²¡æœ‰åˆ†çº¢é…è‚¡æ•°æ®ï¼Œæ·»åŠ ç©ºåˆ—
                weekly_data['dividend_amount'] = 0.0
                weekly_data['allotment_ratio'] = 0.0
                weekly_data['allotment_price'] = 0.0
                weekly_data['bonus_ratio'] = 0.0
                weekly_data['transfer_ratio'] = 0.0
                return weekly_data
            
            # ç¡®ä¿ç´¢å¼•æ˜¯æ—¥æœŸç±»å‹
            weekly_data.index = pd.to_datetime(weekly_data.index)
            
            # åˆå§‹åŒ–åˆ†çº¢é…è‚¡åˆ—
            weekly_data['dividend_amount'] = 0.0
            weekly_data['allotment_ratio'] = 0.0
            weekly_data['allotment_price'] = 0.0
            weekly_data['bonus_ratio'] = 0.0
            weekly_data['transfer_ratio'] = 0.0
            
            # å°†åˆ†çº¢é…è‚¡æ—¥æœŸæ˜ å°„åˆ°å¯¹åº”çš„å‘¨çº¿æ—¥æœŸ
            for ex_date, dividend_row in dividend_data.iterrows():
                try:
                    # ex_date å·²ç»æ˜¯ç´¢å¼•ï¼Œä¸éœ€è¦ä» dividend_row ä¸­è·å–
                    
                    # ç¡®ä¿ ex_date æ˜¯ Timestamp ç±»å‹ï¼Œå¹¶ç§»é™¤æ—¶åŒºä¿¡æ¯
                    if hasattr(ex_date, 'tz_localize'):
                        ex_date = ex_date.tz_localize(None) if ex_date.tz is not None else ex_date
                    else:
                        ex_date = pd.Timestamp(ex_date)
                    
                    # æ‰¾åˆ°æœ€æ¥è¿‘çš„å‘¨çº¿æ—¥æœŸï¼ˆé€šå¸¸æ˜¯å½“å‘¨æˆ–ä¸‹å‘¨çš„å‘¨äº”ï¼‰
                    # æ‰¾åˆ°é™¤æƒé™¤æ¯æ—¥æ‰€åœ¨å‘¨çš„å‘¨äº”ï¼Œå¦‚æœé™¤æƒæ—¥åœ¨å‘¨äº”ä¹‹åï¼Œåˆ™æ˜ å°„åˆ°ä¸‹å‘¨äº”
                    try:
                        weekday = ex_date.weekday()  # 0=Monday, 4=Friday
                        
                        if weekday <= 4:  # å‘¨ä¸€åˆ°å‘¨äº”
                            # æ˜ å°„åˆ°å½“å‘¨å‘¨äº”
                            days_to_friday = 4 - weekday
                            target_friday = ex_date + pd.Timedelta(days=days_to_friday)
                        else:  # å‘¨å…­å‘¨æ—¥
                            # æ˜ å°„åˆ°ä¸‹å‘¨å‘¨äº”
                            days_to_next_friday = 4 + (7 - weekday)
                            target_friday = ex_date + pd.Timedelta(days=days_to_next_friday)
                    except Exception as date_calc_e:
                        # å¦‚æœè®¡ç®—target_fridayå¤±è´¥ï¼Œè·³è¿‡è¿™æ¡åˆ†çº¢è®°å½•
                        logger.debug(f"è®¡ç®—target_fridayå¤±è´¥ï¼Œè·³è¿‡åˆ†çº¢è®°å½•: ex_date={ex_date}")
                        continue
                    
                    # æ‰¾åˆ°æœ€æ¥è¿‘çš„å‘¨çº¿æ•°æ®æ—¥æœŸ
                    closest_date = None
                    min_diff = float('inf')
                    
                    for week_date in weekly_data.index:
                        try:
                            # ç¡®ä¿ week_date ä¹Ÿæ˜¯ Timestamp ç±»å‹ï¼Œå¹¶ç§»é™¤æ—¶åŒºä¿¡æ¯
                            if hasattr(week_date, 'tz_localize'):
                                week_date_normalized = week_date.tz_localize(None) if week_date.tz is not None else week_date
                            else:
                                week_date_normalized = pd.Timestamp(week_date)
                            
                            # è®¡ç®—æ—¥æœŸå·®å¼‚ï¼ˆä½¿ç”¨total_secondsé¿å…å¼‚å¸¸Timedeltaçš„.dayså±æ€§é—®é¢˜ï¼‰
                            try:
                                time_diff = week_date_normalized - target_friday
                                # ä½¿ç”¨total_seconds()è½¬æ¢ä¸ºå¤©æ•°ï¼Œæ›´åŠ ç¨³å®š
                                diff_days = abs(time_diff.total_seconds() / 86400)  # 86400ç§’ = 1å¤©
                            except (AttributeError, OverflowError, ValueError) as calc_e:
                                # å¦‚æœæ—¥æœŸè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ—¥æœŸ
                                continue
                            
                            if diff_days < min_diff:
                                min_diff = diff_days
                                closest_date = week_date
                        except Exception as inner_e:
                            logger.warning(f"è®¡ç®—æ—¥æœŸå·®å¼‚å¤±è´¥ï¼Œè·³è¿‡æ­¤æ—¥æœŸ: week_date={week_date}, target_friday={target_friday}")
                            continue
                    
                    # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„æ—¥æœŸï¼Œæ›´æ–°åˆ†çº¢é…è‚¡ä¿¡æ¯
                    if closest_date is not None and min_diff <= 7:  # å…è®¸7å¤©å†…çš„è¯¯å·®
                        weekly_data.loc[closest_date, 'dividend_amount'] = dividend_row.get('dividend_amount', 0)
                        weekly_data.loc[closest_date, 'allotment_ratio'] = dividend_row.get('allotment_ratio', 0)
                        weekly_data.loc[closest_date, 'allotment_price'] = dividend_row.get('allotment_price', 0)
                        weekly_data.loc[closest_date, 'bonus_ratio'] = dividend_row.get('bonus_ratio', 0)
                        weekly_data.loc[closest_date, 'transfer_ratio'] = dividend_row.get('transfer_ratio', 0)
                        
                        logger.debug(f"åˆ†çº¢é…è‚¡ä¿¡æ¯å·²å¯¹é½: {ex_date.date()} -> {closest_date.date()}")
                except Exception as row_e:
                    # å°†è­¦å‘Šæ”¹ä¸ºè°ƒè¯•çº§åˆ«ï¼Œé¿å…å¤§é‡è­¦å‘Šä¿¡æ¯
                    # è¿™äº›å¼‚å¸¸é€šå¸¸æ˜¯ç”±äºæ—¥æœŸè®¡ç®—é—®é¢˜å¯¼è‡´çš„ï¼Œä¸å½±å“ä¸»è¦åŠŸèƒ½
                    logger.debug(f"å¤„ç†åˆ†çº¢è®°å½•å¤±è´¥ ex_date={ex_date}: {type(row_e).__name__}")
                    continue
            
            return weekly_data
            
        except Exception as e:
            logger.error(f"åˆ†çº¢é…è‚¡æ•°æ®å¯¹é½å¤±è´¥: {str(e)}")
            # è¿”å›åŸå§‹æ•°æ®ï¼Œæ·»åŠ ç©ºçš„åˆ†çº¢é…è‚¡åˆ—
            weekly_data['dividend_amount'] = 0.0
            weekly_data['allotment_ratio'] = 0.0
            weekly_data['allotment_price'] = 0.0
            weekly_data['bonus_ratio'] = 0.0
            weekly_data['transfer_ratio'] = 0.0
            return weekly_data
    
    def _process_dividend_data(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        """
        å¤„ç†åŸå§‹åˆ†çº¢é…è‚¡æ•°æ®
        
        Args:
            raw_data: åŸå§‹åˆ†çº¢æ•°æ®
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„åˆ†çº¢æ•°æ®ï¼Œä»¥é™¤æƒæ—¥ä¸ºç´¢å¼•
        """
        if raw_data is None or raw_data.empty:
            return pd.DataFrame()
        
        try:
            # åˆ›å»ºæ ‡å‡†åŒ–çš„åˆ†çº¢æ•°æ®ç»“æ„
            processed_data = pd.DataFrame()
            
            # æ ¹æ®å®é™…çš„åˆ—åè¿›è¡Œæ˜ å°„å’Œå¤„ç†
            if 'é™¤æƒé™¤æ¯æ—¥' in raw_data.columns:
                processed_data['ex_date'] = pd.to_datetime(raw_data['é™¤æƒé™¤æ¯æ—¥'])
            elif 'é™¤æƒæ—¥' in raw_data.columns:
                processed_data['ex_date'] = pd.to_datetime(raw_data['é™¤æƒæ—¥'])
            elif 'ex_date' in raw_data.columns:
                processed_data['ex_date'] = pd.to_datetime(raw_data['ex_date'])
            else:
                logger.warning("æœªæ‰¾åˆ°é™¤æƒæ—¥åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºæ—¥æœŸ")
                processed_data['ex_date'] = pd.to_datetime(raw_data.iloc[:, 0])
            
            # åˆ†çº¢é‡‘é¢ (æ´¾æ¯) - æ³¨æ„ï¼šakshareè¿”å›çš„é€šå¸¸æ˜¯æ¯10è‚¡åˆ†çº¢é‡‘é¢ï¼Œéœ€è¦é™¤ä»¥10è½¬æ¢ä¸ºæ¯è‚¡é‡‘é¢
            if 'æ´¾æ¯' in raw_data.columns:
                processed_data['dividend_amount'] = pd.to_numeric(raw_data['æ´¾æ¯'], errors='coerce').fillna(0) / 10.0
            elif 'åˆ†çº¢é‡‘é¢' in raw_data.columns:
                processed_data['dividend_amount'] = pd.to_numeric(raw_data['åˆ†çº¢é‡‘é¢'], errors='coerce').fillna(0) / 10.0
            elif 'dividend' in raw_data.columns:
                processed_data['dividend_amount'] = pd.to_numeric(raw_data['dividend'], errors='coerce').fillna(0) / 10.0
            else:
                processed_data['dividend_amount'] = 0
            
            # é€è‚¡æ¯”ä¾‹ - æ³¨æ„ï¼šakshareè¿”å›çš„æ˜¯æ¯10è‚¡é€Xè‚¡ï¼Œéœ€è¦é™¤ä»¥10è½¬æ¢ä¸ºæ¯è‚¡é€è‚¡æ¯”ä¾‹
            if 'é€è‚¡' in raw_data.columns:
                processed_data['bonus_ratio'] = pd.to_numeric(raw_data['é€è‚¡'], errors='coerce').fillna(0) / 10.0
            elif 'é€è‚¡æ¯”ä¾‹' in raw_data.columns:
                processed_data['bonus_ratio'] = pd.to_numeric(raw_data['é€è‚¡æ¯”ä¾‹'], errors='coerce').fillna(0) / 10.0
            elif 'bonus' in raw_data.columns:
                processed_data['bonus_ratio'] = pd.to_numeric(raw_data['bonus'], errors='coerce').fillna(0) / 10.0
            else:
                processed_data['bonus_ratio'] = 0
            
            # è½¬å¢æ¯”ä¾‹ - æ³¨æ„ï¼šakshareè¿”å›çš„æ˜¯æ¯10è‚¡è½¬å¢Xè‚¡ï¼Œéœ€è¦é™¤ä»¥10è½¬æ¢ä¸ºæ¯è‚¡è½¬å¢æ¯”ä¾‹
            if 'è½¬å¢' in raw_data.columns:
                processed_data['transfer_ratio'] = pd.to_numeric(raw_data['è½¬å¢'], errors='coerce').fillna(0) / 10.0
            elif 'è½¬å¢æ¯”ä¾‹' in raw_data.columns:
                processed_data['transfer_ratio'] = pd.to_numeric(raw_data['è½¬å¢æ¯”ä¾‹'], errors='coerce').fillna(0) / 10.0
            elif 'transfer' in raw_data.columns:
                processed_data['transfer_ratio'] = pd.to_numeric(raw_data['transfer'], errors='coerce').fillna(0)
            else:
                processed_data['transfer_ratio'] = 0
            
            # é…è‚¡æ¯”ä¾‹å’Œä»·æ ¼ (æš‚æ—¶è®¾ä¸º0ï¼Œå› ä¸ºåŸå§‹æ•°æ®ä¸­æ²¡æœ‰è¿™äº›å­—æ®µ)
            processed_data['allotment_ratio'] = 0
            processed_data['allotment_price'] = 0
            
            # è®¾ç½®é™¤æƒæ—¥ä¸ºç´¢å¼•
            processed_data.set_index('ex_date', inplace=True)
            processed_data.sort_index(inplace=True)
            
            # è¿‡æ»¤æ‰æ‰€æœ‰å€¼éƒ½ä¸º0çš„è¡Œ
            mask = (processed_data['dividend_amount'] > 0) | \
                   (processed_data['bonus_ratio'] > 0) | \
                   (processed_data['transfer_ratio'] > 0) | \
                   (processed_data['allotment_ratio'] > 0)
            processed_data = processed_data[mask]
            
            logger.info(f"å¤„ç†åˆ†çº¢æ•°æ®å®Œæˆï¼Œæœ‰æ•ˆè®°å½•æ•°: {len(processed_data)}")
            return processed_data
            
        except Exception as e:
            logger.error(f"å¤„ç†åˆ†çº¢æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

class TushareDataFetcher(DataFetcher):
    """Tushareæ•°æ®è·å–å™¨å®ç°"""
    
    def __init__(self, token: str):
        """
        åˆå§‹åŒ–Tushareæ•°æ®è·å–å™¨
        
        Args:
            token: Tushare API Token
        """
        if not TUSHARE_AVAILABLE:
            raise DataFetchError("Tushareæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tushare")
        
        if not token:
            raise DataFetchError("ä½¿ç”¨Tushareéœ€è¦æä¾›token")
        
        self.source_name = "tushare"
        self.token = token
        self.pro = ts.pro_api(token)
        self.last_request_time = None
        self.min_request_interval = 0.35  # 200æ¬¡/åˆ†é’Ÿ = æ¯æ¬¡é—´éš”0.3ç§’ï¼Œç•™0.05ç§’å®‰å…¨ä½™é‡
        logger.info("åˆå§‹åŒ–Tushareæ•°æ®è·å–å™¨")
    
    def _convert_stock_code(self, code: str) -> str:
        """
        è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼: 601088 -> 601088.SH
        
        Args:
            code: 6ä½è‚¡ç¥¨ä»£ç 
            
        Returns:
            str: Tushareæ ¼å¼çš„è‚¡ç¥¨ä»£ç 
        """
        if code.startswith('6'):
            return f"{code}.SH"
        elif code.startswith('0') or code.startswith('3'):
            return f"{code}.SZ"
        else:
            raise ValueError(f"æ— æ³•è¯†åˆ«çš„è‚¡ç¥¨ä»£ç : {code}")
    
    def _convert_date_format(self, date_str: str) -> str:
        """
        è½¬æ¢æ—¥æœŸæ ¼å¼: 'YYYY-MM-DD' -> 'YYYYMMDD'
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            
        Returns:
            str: Tushareæ ¼å¼çš„æ—¥æœŸ
        """
        return date_str.replace('-', '')
    
    def get_stock_data(self, code: str, start_date: str, end_date: str = None, 
                      period: str = 'weekly') -> pd.DataFrame:
        """
        ä»Tushareè·å–è‚¡ç¥¨å†å²æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç  (å¦‚ '601088')
            start_date: å¼€å§‹æ—¥æœŸ ('YYYY-MM-DD')
            end_date: ç»“æŸæ—¥æœŸ ('YYYY-MM-DD', Noneè¡¨ç¤ºå½“å‰æ—¥æœŸ)
            period: æ•°æ®å‘¨æœŸ ('daily', 'weekly', 'monthly')
            
        Returns:
            pd.DataFrame: æ ‡å‡†åŒ–çš„è‚¡ç¥¨æ•°æ®
            
        Raises:
            DataFetchError: æ•°æ®è·å–å¤±è´¥
        """
        try:
            # å‚æ•°éªŒè¯
            if not code or len(code) != 6:
                raise DataFetchError(f"æ— æ•ˆçš„è‚¡ç¥¨ä»£ç : {code}")
            
            # è½¬æ¢è‚¡ç¥¨ä»£ç å’Œæ—¥æœŸæ ¼å¼
            ts_code = self._convert_stock_code(code)
            ts_start_date = self._convert_date_format(start_date)
            
            if end_date is None:
                end_date = datetime.now().strftime('%Y-%m-%d')
            ts_end_date = self._convert_date_format(end_date)
            
            logger.debug(f"ä»Tushareè·å–æ•°æ®: {ts_code}, {ts_start_date}-{ts_end_date}")
            
            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
            import time
            if self.last_request_time is not None:
                elapsed = time.time() - self.last_request_time
                if elapsed < self.min_request_interval:
                    sleep_time = self.min_request_interval - elapsed
                    logger.debug(f"è¯·æ±‚é—´éš”æ§åˆ¶ï¼šç­‰å¾… {sleep_time:.2f} ç§’")
                    time.sleep(sleep_time)
            
            # æ›´æ–°è¯·æ±‚æ—¶é—´
            self.last_request_time = time.time()
            
            # Tushareåªæä¾›æ—¥çº¿æ•°æ®ï¼Œå‘¨çº¿éœ€è¦ä»æ—¥çº¿é‡é‡‡æ ·
            # è°ƒç”¨dailyæ¥å£è·å–æ—¥çº¿æ•°æ®ï¼ˆä¸å¤æƒï¼‰
            logger.debug(f"è°ƒç”¨Tushare API: ts_code={ts_code}, start_date={ts_start_date}, end_date={ts_end_date}")
            df = self.pro.daily(
                ts_code=ts_code,
                start_date=ts_start_date,
                end_date=ts_end_date
            )
            
            logger.debug(f"Tushare APIè¿”å›: dfç±»å‹={type(df)}, æ˜¯å¦ä¸ºNone={df is None}, æ˜¯å¦ä¸ºç©º={df.empty if df is not None else 'N/A'}")
            if df is not None and not df.empty:
                logger.debug(f"è¿”å›æ•°æ®å½¢çŠ¶: {df.shape}, åˆ—å: {list(df.columns)}")
            
            if df is None or df.empty:
                # è¿”å›ç©ºDataFrameè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸ï¼Œè®©å›æµ‹å¼•æ“çš„é™çº§é‡è¯•æœºåˆ¶å¤„ç†
                # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨è¯·æ±‚éäº¤æ˜“æ—¥æˆ–æ•°æ®ä¸å­˜åœ¨æ—¶
                logger.warning(f"Tushareæœªè¿”å›è‚¡ç¥¨ {code} çš„æ•°æ®ï¼ˆå¯èƒ½æ˜¯éäº¤æ˜“æ—¥æˆ–æ•°æ®ä¸å­˜åœ¨ï¼‰: {start_date} åˆ° {end_date}")
                return pd.DataFrame()
            
            logger.debug(f"æˆåŠŸè·å–è‚¡ç¥¨ {code} æ—¥çº¿æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
            
            # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
            df = self._standardize_data_format(df)
            
            # å¦‚æœéœ€è¦å‘¨çº¿æ•°æ®ï¼Œä»æ—¥çº¿é‡é‡‡æ ·
            if period == 'weekly':
                from .data_processor import DataProcessor
                processor = DataProcessor()
                df = processor.resample_to_weekly(df)
                logger.debug(f"æ—¥çº¿è½¬å‘¨çº¿å®Œæˆï¼Œå…± {len(df)} æ¡å‘¨çº¿è®°å½•")
            elif period == 'monthly':
                from .data_processor import DataProcessor
                processor = DataProcessor()
                df = processor.resample_to_monthly(df)
                logger.debug(f"æ—¥çº¿è½¬æœˆçº¿å®Œæˆï¼Œå…± {len(df)} æ¡æœˆçº¿è®°å½•")
            
            logger.debug(f"æˆåŠŸè·å–è‚¡ç¥¨ {code} æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
            return df
            
        except Exception as e:
            error_msg = f"è·å–è‚¡ç¥¨ {code} æ•°æ®å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            raise DataFetchError(error_msg) from e
    
    def _standardize_data_format(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ‡å‡†åŒ–Tushareæ•°æ®æ ¼å¼ï¼Œç¡®ä¿ä¸AkshareDataFetcherè¾“å‡ºä¸€è‡´
        
        Args:
            df: åŸå§‹Tushareæ•°æ®
            
        Returns:
            pd.DataFrame: æ ‡å‡†åŒ–åçš„æ•°æ®
        """
        try:
            # Tushareåˆ—åæ˜ å°„
            column_mapping = {
                'trade_date': 'date',
                'open': 'open',
                'high': 'high',
                'low': 'low',
                'close': 'close',
                'vol': 'volume',      # æˆäº¤é‡ï¼ˆæ‰‹ï¼‰
                'amount': 'amount'    # æˆäº¤é¢ï¼ˆåƒå…ƒï¼‰
            }
            
            # é‡å‘½ååˆ—
            df = df.rename(columns=column_mapping)
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼: YYYYMMDD -> datetime
            df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')
            
            # è®¾ç½®æ—¥æœŸä¸ºç´¢å¼•
            df = df.set_index('date')
            
            # å•ä½è½¬æ¢
            # Tushareæˆäº¤é‡å•ä½æ˜¯æ‰‹ï¼ˆ1æ‰‹=100è‚¡ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºè‚¡
            df['volume'] = df['volume'] * 100
            
            # Tushareæˆäº¤é¢å•ä½æ˜¯åƒå…ƒï¼Œéœ€è¦è½¬æ¢ä¸ºå…ƒ
            if 'amount' in df.columns:
                df['amount'] = df['amount'] * 1000
            
            # ç¡®ä¿æ•°å€¼åˆ—ä¸ºfloatç±»å‹
            numeric_columns = ['open', 'high', 'low', 'close', 'volume']
            if 'amount' in df.columns:
                numeric_columns.append('amount')
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # æŒ‰æ—¥æœŸæ’åºï¼ˆTushareè¿”å›çš„æ•°æ®æ˜¯å€’åºçš„ï¼‰
            df = df.sort_index()
            
            # é€‰æ‹©éœ€è¦çš„åˆ—
            output_columns = ['open', 'high', 'low', 'close', 'volume']
            if 'amount' in df.columns:
                output_columns.append('amount')
            
            df = df[output_columns]
            
            return df
            
        except Exception as e:
            raise DataFetchError(f"æ•°æ®æ ¼å¼æ ‡å‡†åŒ–å¤±è´¥: {str(e)}") from e
    
    def get_dividend_data(self, code: str, start_date: str, end_date: str = None,
                         use_cache: bool = True) -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨åˆ†çº¢é…è‚¡æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç  (å¦‚ '601088')
            start_date: å¼€å§‹æ—¥æœŸ ('YYYY-MM-DD')
            end_date: ç»“æŸæ—¥æœŸ ('YYYY-MM-DD', Noneè¡¨ç¤ºå½“å‰æ—¥æœŸ)
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            pd.DataFrame: åˆ†çº¢é…è‚¡æ•°æ®
        """
        try:
            # å¯¼å…¥æ•°æ®å­˜å‚¨æ¨¡å—
            from .data_storage import DataStorage
            storage = DataStorage()
            
            # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œå…ˆæ£€æŸ¥ç¼“å­˜
            if use_cache:
                if storage.is_dividend_date_range_cached(code, start_date or '1990-01-01',
                                                       end_date or datetime.now().strftime('%Y-%m-%d')):
                    cached_data = storage.load_dividend_data(code)
                    if cached_data is not None:
                        logger.info(f"ğŸ“¦ ä½¿ç”¨åˆ†çº¢é…è‚¡ç¼“å­˜æ•°æ®: {code}")
                        
                        # æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤
                        filtered_data = cached_data.copy()
                        if start_date:
                            start_dt = pd.to_datetime(start_date)
                            filtered_data = filtered_data[filtered_data.index >= start_dt]
                        if end_date:
                            end_dt = pd.to_datetime(end_date)
                            filtered_data = filtered_data[filtered_data.index <= end_dt]
                        
                        logger.info(f"âœ… ç¼“å­˜åˆ†çº¢é…è‚¡æ•°æ®è¿‡æ»¤å: {code}, {len(filtered_data)} æ¡è®°å½•")
                        return filtered_data
            
            # ä»ç½‘ç»œè·å–
            logger.info(f"ğŸŒ ä»Tushareè·å–è‚¡ç¥¨ {code} çš„åˆ†çº¢é…è‚¡æ•°æ®...")
            
            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
            import time
            if self.last_request_time is not None:
                elapsed = time.time() - self.last_request_time
                if elapsed < self.min_request_interval:
                    sleep_time = self.min_request_interval - elapsed
                    time.sleep(sleep_time)
            
            self.last_request_time = time.time()
            
            # è½¬æ¢è‚¡ç¥¨ä»£ç 
            ts_code = self._convert_stock_code(code)
            
            # è°ƒç”¨Tushareåˆ†çº¢æ¥å£ï¼ˆæ³¨æ„ï¼šdividendæ¥å£ä¸æ”¯æŒstart_date/end_dateå‚æ•°ï¼‰
            # éœ€è¦è·å–å…¨éƒ¨æ•°æ®ï¼Œç„¶ååœ¨å¤„ç†åæŒ‰æ—¥æœŸè¿‡æ»¤
            dividend_data = self.pro.dividend(ts_code=ts_code)
            
            if dividend_data is None or dividend_data.empty:
                logger.warning(f"æœªè·å–åˆ°è‚¡ç¥¨ {code} çš„åˆ†çº¢é…è‚¡æ•°æ®")
                if use_cache:
                    empty_df = pd.DataFrame()
                    storage.save_dividend_data(empty_df, code)
                return pd.DataFrame()
            
            # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
            processed_data = self._process_dividend_data(dividend_data)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if use_cache and not processed_data.empty:
                storage.save_dividend_data(processed_data, code)
                logger.info(f"ğŸ’¾ åˆ†çº¢é…è‚¡æ•°æ®å·²ç¼“å­˜: {code}")
            
            # æŒ‰æ—¥æœŸèŒƒå›´è¿‡æ»¤
            filtered_data = processed_data.copy()
            if start_date:
                start_dt = pd.to_datetime(start_date)
                filtered_data = filtered_data[filtered_data.index >= start_dt]
            if end_date:
                end_dt = pd.to_datetime(end_date)
                filtered_data = filtered_data[filtered_data.index <= end_dt]
            
            logger.info(f"æˆåŠŸè·å–è‚¡ç¥¨ {code} çš„åˆ†çº¢é…è‚¡æ•°æ®ï¼Œå…± {len(filtered_data)} æ¡è®°å½•")
            return filtered_data
            
        except Exception as e:
            error_msg = f"è·å–è‚¡ç¥¨ {code} åˆ†çº¢é…è‚¡æ•°æ®å¤±è´¥: {str(e)}"
            logger.warning(error_msg)
            return pd.DataFrame()
    
    def _process_dividend_data(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        """
        å¤„ç†TushareåŸå§‹åˆ†çº¢é…è‚¡æ•°æ®
        
        Args:
            raw_data: åŸå§‹åˆ†çº¢æ•°æ®
            
        Returns:
            pd.DataFrame: å¤„ç†åçš„åˆ†çº¢æ•°æ®ï¼Œä»¥é™¤æƒæ—¥ä¸ºç´¢å¼•
        """
        if raw_data is None or raw_data.empty:
            return pd.DataFrame()
        
        try:
            processed_data = pd.DataFrame()
            
            # Tushareåˆ†çº¢æ•°æ®å­—æ®µï¼š
            # ex_date: é™¤æƒé™¤æ¯æ—¥
            # cash_div: æ¯è‚¡åˆ†çº¢ï¼ˆç¨å‰ï¼‰
            # stk_bo_rate: é€è‚¡æ¯”ä¾‹ï¼ˆæ¯10è‚¡é€Xè‚¡ï¼‰
            # stk_co_rate: è½¬å¢æ¯”ä¾‹ï¼ˆæ¯10è‚¡è½¬å¢Xè‚¡ï¼‰
            
            # é™¤æƒæ—¥
            if 'ex_date' in raw_data.columns:
                processed_data['ex_date'] = pd.to_datetime(raw_data['ex_date'], format='%Y%m%d')
            else:
                logger.warning("æœªæ‰¾åˆ°é™¤æƒæ—¥åˆ—")
                return pd.DataFrame()
            
            # åˆ†çº¢é‡‘é¢ï¼ˆæ¯è‚¡ï¼‰- Tushareå·²ç»æ˜¯æ¯è‚¡é‡‘é¢ï¼Œä¸éœ€è¦é™¤ä»¥10
            if 'cash_div' in raw_data.columns:
                processed_data['dividend_amount'] = pd.to_numeric(raw_data['cash_div'], errors='coerce').fillna(0)
            else:
                processed_data['dividend_amount'] = 0
            
            # é€è‚¡æ¯”ä¾‹ - Tushareæ˜¯æ¯10è‚¡é€Xè‚¡ï¼Œéœ€è¦é™¤ä»¥10è½¬æ¢ä¸ºæ¯è‚¡æ¯”ä¾‹
            if 'stk_bo_rate' in raw_data.columns:
                processed_data['bonus_ratio'] = pd.to_numeric(raw_data['stk_bo_rate'], errors='coerce').fillna(0) / 10.0
            else:
                processed_data['bonus_ratio'] = 0
            
            # è½¬å¢æ¯”ä¾‹ - Tushareæ˜¯æ¯10è‚¡è½¬å¢Xè‚¡ï¼Œéœ€è¦é™¤ä»¥10
            if 'stk_co_rate' in raw_data.columns:
                processed_data['transfer_ratio'] = pd.to_numeric(raw_data['stk_co_rate'], errors='coerce').fillna(0) / 10.0
            else:
                processed_data['transfer_ratio'] = 0
            
            # é…è‚¡æ¯”ä¾‹å’Œä»·æ ¼ï¼ˆæš‚æ—¶è®¾ä¸º0ï¼‰
            processed_data['allotment_ratio'] = 0
            processed_data['allotment_price'] = 0
            
            # è®¾ç½®é™¤æƒæ—¥ä¸ºç´¢å¼•
            processed_data.set_index('ex_date', inplace=True)
            processed_data.sort_index(inplace=True)
            
            # è¿‡æ»¤æ‰æ‰€æœ‰å€¼éƒ½ä¸º0çš„è¡Œ
            mask = (processed_data['dividend_amount'] > 0) | \
                   (processed_data['bonus_ratio'] > 0) | \
                   (processed_data['transfer_ratio'] > 0) | \
                   (processed_data['allotment_ratio'] > 0)
            processed_data = processed_data[mask]
            
            logger.info(f"å¤„ç†Tushareåˆ†çº¢æ•°æ®å®Œæˆï¼Œæœ‰æ•ˆè®°å½•æ•°: {len(processed_data)}")
            return processed_data
            
        except Exception as e:
            logger.error(f"å¤„ç†Tushareåˆ†çº¢æ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def align_dividend_with_weekly_data(self, weekly_data: pd.DataFrame,
                                      dividend_data: pd.DataFrame) -> pd.DataFrame:
        """
        å°†åˆ†çº¢é…è‚¡æ•°æ®ä¸å‘¨çº¿æ•°æ®å¯¹é½
        
        Args:
            weekly_data: å‘¨çº¿æ•°æ®
            dividend_data: åˆ†çº¢é…è‚¡æ•°æ®
            
        Returns:
            pd.DataFrame: å¯¹é½åçš„å‘¨çº¿æ•°æ®ï¼ˆåŒ…å«åˆ†çº¢é…è‚¡ä¿¡æ¯ï¼‰
        """
        try:
            # å¦‚æœæ²¡æœ‰åˆ†çº¢æ•°æ®ï¼Œæ·»åŠ ç©ºåˆ—å¹¶è¿”å›
            if dividend_data is None or dividend_data.empty:
                weekly_data['dividend_amount'] = 0.0
                weekly_data['allotment_ratio'] = 0.0
                weekly_data['allotment_price'] = 0.0
                weekly_data['bonus_ratio'] = 0.0
                weekly_data['transfer_ratio'] = 0.0
                return weekly_data
            
            # åˆå§‹åŒ–åˆ†çº¢é…è‚¡åˆ—
            weekly_data['dividend_amount'] = 0.0
            weekly_data['allotment_ratio'] = 0.0
            weekly_data['allotment_price'] = 0.0
            weekly_data['bonus_ratio'] = 0.0
            weekly_data['transfer_ratio'] = 0.0
            
            # éå†æ¯ä¸ªåˆ†çº¢äº‹ä»¶
            for ex_date, dividend_row in dividend_data.iterrows():
                try:
                    # ç¡®ä¿ ex_date æ˜¯ Timestamp ç±»å‹ï¼Œå¹¶ç§»é™¤æ—¶åŒºä¿¡æ¯
                    try:
                        if hasattr(ex_date, 'tz_localize'):
                            ex_date_normalized = ex_date.tz_localize(None) if ex_date.tz is not None else ex_date
                        else:
                            ex_date_normalized = pd.Timestamp(ex_date)
                    except Exception as norm_e:
                        # å¦‚æœæ—¥æœŸæ ‡å‡†åŒ–å¤±è´¥ï¼Œè·³è¿‡è¿™æ¡åˆ†çº¢è®°å½•
                        logger.debug(f"æ—¥æœŸæ ‡å‡†åŒ–å¤±è´¥ï¼Œè·³è¿‡åˆ†çº¢è®°å½•: ex_date={ex_date}")
                        continue
                    
                    # æ‰¾åˆ°æœ€æ¥è¿‘çš„å‘¨çº¿æ—¥æœŸ
                    closest_date = None
                    min_diff = pd.Timedelta(days=30)  # ä½¿ç”¨åˆç†çš„åˆå§‹å€¼é¿å…æº¢å‡º
                    
                    for week_date in weekly_data.index:
                        try:
                            # ç¡®ä¿ week_date ä¹Ÿæ˜¯ Timestamp ç±»å‹ï¼Œå¹¶ç§»é™¤æ—¶åŒºä¿¡æ¯
                            if hasattr(week_date, 'tz_localize'):
                                week_date_normalized = week_date.tz_localize(None) if week_date.tz is not None else week_date
                            else:
                                week_date_normalized = pd.Timestamp(week_date)
                            
                            # è®¡ç®—æ—¥æœŸå·®å¼‚ï¼ˆä½¿ç”¨total_secondsé¿å…å¼‚å¸¸Timedeltaé—®é¢˜ï¼‰
                            try:
                                time_diff = week_date_normalized - ex_date_normalized
                                # è½¬æ¢ä¸ºå¤©æ•°è¿›è¡Œæ¯”è¾ƒ
                                diff_days = abs(time_diff.total_seconds() / 86400)
                                diff = pd.Timedelta(days=diff_days)
                            except (AttributeError, OverflowError, ValueError) as calc_e:
                                # å¦‚æœæ—¥æœŸè®¡ç®—å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ—¥æœŸ
                                continue
                            
                            if diff < min_diff:
                                min_diff = diff
                                closest_date = week_date
                        except Exception as inner_e:
                            logger.warning(f"è®¡ç®—æ—¥æœŸå·®å¼‚å¤±è´¥ï¼Œè·³è¿‡æ­¤æ—¥æœŸ: week_date={week_date}, ex_date={ex_date}")
                            continue
                    
                    # å¦‚æœæ‰¾åˆ°åŒ¹é…çš„å‘¨çº¿æ—¥æœŸï¼ˆå…è®¸7å¤©å†…çš„å·®å¼‚ï¼‰
                    if closest_date is not None and min_diff <= pd.Timedelta(days=7):
                        weekly_data.loc[closest_date, 'dividend_amount'] = dividend_row.get('dividend_amount', 0)
                        weekly_data.loc[closest_date, 'allotment_ratio'] = dividend_row.get('allotment_ratio', 0)
                        weekly_data.loc[closest_date, 'allotment_price'] = dividend_row.get('allotment_price', 0)
                        weekly_data.loc[closest_date, 'bonus_ratio'] = dividend_row.get('bonus_ratio', 0)
                        weekly_data.loc[closest_date, 'transfer_ratio'] = dividend_row.get('transfer_ratio', 0)
                        
                        logger.debug(f"åˆ†çº¢é…è‚¡ä¿¡æ¯å·²å¯¹é½: {ex_date_normalized.date()} -> {closest_date.date()}")
                except Exception as row_e:
                    # å°†è­¦å‘Šæ”¹ä¸ºè°ƒè¯•çº§åˆ«ï¼Œé¿å…å¤§é‡è­¦å‘Šä¿¡æ¯
                    # è¿™äº›å¼‚å¸¸é€šå¸¸æ˜¯ç”±äºæ—¥æœŸè®¡ç®—é—®é¢˜å¯¼è‡´çš„ï¼Œä¸å½±å“ä¸»è¦åŠŸèƒ½
                    logger.debug(f"å¤„ç†åˆ†çº¢è®°å½•å¤±è´¥ ex_date={ex_date}: {type(row_e).__name__}")
                    continue
            
            return weekly_data
            
        except Exception as e:
            logger.error(f"åˆ†çº¢é…è‚¡æ•°æ®å¯¹é½å¤±è´¥: {str(e)}")
            # è¿”å›åŸå§‹æ•°æ®ï¼Œæ·»åŠ ç©ºçš„åˆ†çº¢é…è‚¡åˆ—
            weekly_data['dividend_amount'] = 0.0
            weekly_data['allotment_ratio'] = 0.0
            weekly_data['allotment_price'] = 0.0
            weekly_data['bonus_ratio'] = 0.0
            weekly_data['transfer_ratio'] = 0.0
            return weekly_data
    
    def test_connection(self) -> bool:
        """
        æµ‹è¯•Tushareè¿æ¥
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æ­£å¸¸
        """
        try:
            # å°è¯•è·å–ä¸€åªè‚¡ç¥¨çš„å°‘é‡æ•°æ®æ¥æµ‹è¯•è¿æ¥
            test_code = "000001.SZ"  # å¹³å®‰é“¶è¡Œ
            end_date = datetime.now().strftime('%Y%m%d')
            start_date = (datetime.now() - timedelta(days=7)).strftime('%Y%m%d')
            
            df = self.pro.daily(
                ts_code=test_code,
                start_date=start_date,
                end_date=end_date
            )
            
            return df is not None and not df.empty
            
        except Exception as e:
            logger.error(f"æµ‹è¯•Tushareè¿æ¥å¤±è´¥: {str(e)}")
            return False


class DataFetcherFactory:
    """æ•°æ®è·å–å™¨å·¥å‚ç±»"""
    
    @staticmethod
    def create_fetcher(source_type: str, config: dict = None) -> DataFetcher:
        """
        æ ¹æ®é…ç½®åˆ›å»ºæ•°æ®è·å–å™¨
        
        Args:
            source_type: æ•°æ®æºç±»å‹ ('akshare' æˆ– 'tushare')
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«:
                - tushare_token: Tushare API Token (ä½¿ç”¨tushareæ—¶å¿…å¡«)
        
        Returns:
            DataFetcher: æ•°æ®è·å–å™¨å®ä¾‹
            
        Raises:
            ValueError: ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹æˆ–ç¼ºå°‘å¿…è¦é…ç½®
        """
        if config is None:
            config = {}
        
        source_type = source_type.lower().strip()
        
        if source_type == 'akshare':
            logger.info("åˆ›å»º Akshare æ•°æ®è·å–å™¨")
            return AkshareDataFetcher()
            
        elif source_type == 'tushare':
            token = config.get('tushare_token')
            if not token:
                # å°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–
                import os
                token = os.getenv('TUSHARE_TOKEN')
                if not token:
                    raise ValueError("ä½¿ç”¨ Tushare æ•°æ®æºéœ€è¦æä¾› tushare_token")
            logger.info("åˆ›å»º Tushare æ•°æ®è·å–å™¨")
            return TushareDataFetcher(token)
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {source_type}ï¼Œæ”¯æŒçš„ç±»å‹: akshare, tushare")
    
    @staticmethod
    def create_with_fallback(primary: str, backup: str, config: dict) -> DataFetcher:
        """
        åˆ›å»ºå¸¦é™çº§çš„æ•°æ®è·å–å™¨
        
        å°è¯•åˆ›å»ºä¸»æ•°æ®æºï¼Œå¦‚æœå¤±è´¥åˆ™è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®æº
        
        Args:
            primary: ä¸»æ•°æ®æºç±»å‹
            backup: å¤‡ç”¨æ•°æ®æºç±»å‹
            config: é…ç½®å­—å…¸
            
        Returns:
            DataFetcher: æˆåŠŸåˆ›å»ºçš„æ•°æ®è·å–å™¨å®ä¾‹
        """
        try:
            logger.info(f"å°è¯•åˆå§‹åŒ–ä¸»æ•°æ®æº: {primary}")
            return DataFetcherFactory.create_fetcher(primary, config)
        except Exception as e:
            logger.warning(f"ä¸»æ•°æ®æº {primary} åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.info(f"åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®æº: {backup}")
            return DataFetcherFactory.create_fetcher(backup, config)


# ä¿ç•™æ—§çš„å·¥å‚å‡½æ•°ä»¥ä¿æŒå‘åå…¼å®¹
def create_data_fetcher(source: str = 'akshare') -> DataFetcher:
    """
    åˆ›å»ºæ•°æ®è·å–å™¨ï¼ˆå‘åå…¼å®¹çš„å·¥å‚å‡½æ•°ï¼‰
    
    Args:
        source: æ•°æ®æºåç§°
        
    Returns:
        DataFetcher: æ•°æ®è·å–å™¨å®ä¾‹
        
    Raises:
        DataFetchError: ä¸æ”¯æŒçš„æ•°æ®æº
    """
    try:
        return DataFetcherFactory.create_fetcher(source)
    except ValueError as e:
        raise DataFetchError(str(e)) from e

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    
    fetcher = AkshareDataFetcher()
    
    # æµ‹è¯•è¿æ¥
    if fetcher.test_connection():
        print("âœ… Akshareè¿æ¥æ­£å¸¸")
    else:
        print("âŒ Akshareè¿æ¥å¤±è´¥")
    
    # æµ‹è¯•è·å–å•åªè‚¡ç¥¨æ•°æ®
    try:
        data = fetcher.get_stock_data('601088', '2023-01-01', '2023-12-31', 'weekly')
        print(f"âœ… æˆåŠŸè·å–ä¸­å›½ç¥åå‘¨çº¿æ•°æ®ï¼Œå…± {len(data)} æ¡è®°å½•")
        print(data.head())
    except Exception as e:
        print(f"âŒ è·å–æ•°æ®å¤±è´¥: {e}")