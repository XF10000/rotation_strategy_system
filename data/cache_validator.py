"""
ç¼“å­˜æ•°æ®éªŒè¯å’Œè‡ªåŠ¨ä¿®å¤æ¨¡å—
é›†æˆåˆ°ä¸»å›æµ‹æµç¨‹ä¸­ï¼Œæä¾›è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤åŠŸèƒ½
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

import pandas as pd

logger = logging.getLogger(__name__)

class CacheValidator:
    """ç¼“å­˜æ•°æ®éªŒè¯å™¨ - è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤æ•°æ®é—®é¢˜"""
    
    def __init__(self, cache_dir: str = 'data_cache'):
        """åˆå§‹åŒ–ç¼“å­˜éªŒè¯å™¨"""
        self.cache_dir = Path(cache_dir)
        self.stock_data_dir = self.cache_dir / 'stock_data'
        self.indicators_dir = self.cache_dir / 'indicators'
        
        # éªŒè¯ç»“æœ
        self.validation_results = {
            'passed': True,
            'issues': [],
            'auto_fixed': [],
            'manual_action_required': []
        }
    
    def validate_and_fix(self, stock_codes: List[str], period: str = 'weekly') -> Dict[str, Any]:
        """
        éªŒè¯ç¼“å­˜æ•°æ®å¹¶è‡ªåŠ¨ä¿®å¤é—®é¢˜
        
        Args:
            stock_codes: éœ€è¦éªŒè¯çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
            period: æ•°æ®å‘¨æœŸ
            
        Returns:
            Dict: éªŒè¯å’Œä¿®å¤ç»“æœ
        """
        logger.info("ğŸ” å¼€å§‹ç¼“å­˜æ•°æ®éªŒè¯å’Œè‡ªåŠ¨ä¿®å¤...")
        
        # é‡ç½®éªŒè¯ç»“æœ
        self.validation_results = {
            'passed': True,
            'issues': [],
            'auto_fixed': [],
            'manual_action_required': []
        }
        
        try:
            # 1. æ£€æŸ¥ç›®å½•ç»“æ„
            self._check_directory_structure()
            
            # 2. éªŒè¯è‚¡ç¥¨æ•°æ®å®Œæ•´æ€§
            self._validate_stock_data(stock_codes, period)
            
            # 3. éªŒè¯æŠ€æœ¯æŒ‡æ ‡æ•°æ®
            self._validate_indicators_data(stock_codes)
            
            # 4. æ£€æŸ¥æ•°æ®æ ¼å¼ä¸€è‡´æ€§
            self._validate_data_format(stock_codes, period)
            
            # 5. æ£€æŸ¥æ•°æ®æ—¶é—´èŒƒå›´
            self._validate_date_ranges(stock_codes, period)
            
            # 6. ç”ŸæˆéªŒè¯æŠ¥å‘Š
            return self._generate_validation_report()
            
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            self.validation_results['passed'] = False
            self.validation_results['issues'].append(f"éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
            return self.validation_results
    
    def _check_directory_structure(self):
        """æ£€æŸ¥å¹¶ä¿®å¤ç›®å½•ç»“æ„"""
        required_dirs = [
            self.cache_dir,
            self.stock_data_dir,
            self.stock_data_dir / 'daily',
            self.stock_data_dir / 'weekly',
            self.stock_data_dir / 'monthly',
            self.indicators_dir
        ]
        
        for dir_path in required_dirs:
            if not dir_path.exists():
                logger.info(f"ğŸ”§ åˆ›å»ºç¼ºå¤±ç›®å½•: {dir_path}")
                dir_path.mkdir(parents=True, exist_ok=True)
                self.validation_results['auto_fixed'].append(f"åˆ›å»ºç›®å½•: {dir_path}")
    
    def _validate_stock_data(self, stock_codes: List[str], period: str):
        """éªŒè¯è‚¡ç¥¨æ•°æ®å®Œæ•´æ€§"""
        logger.info(f"ğŸ“Š éªŒè¯ {len(stock_codes)} åªè‚¡ç¥¨çš„ {period} æ•°æ®...")
        
        for code in stock_codes:
            try:
                # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                data_file = self.stock_data_dir / period / f"{code}.csv"
                metadata_file = self.stock_data_dir / period / f"{code}.json"
                
                if not data_file.exists():
                    issue = f"è‚¡ç¥¨æ•°æ®æ–‡ä»¶ç¼ºå¤±: {code} ({period})"
                    self.validation_results['issues'].append(issue)
                    self.validation_results['passed'] = False
                    continue
                
                # æ£€æŸ¥æ•°æ®æ˜¯å¦å¯è¯»
                try:
                    data = pd.read_csv(data_file, index_col=0, parse_dates=True)
                    
                    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
                    required_columns = ['open', 'high', 'low', 'close', 'volume']
                    missing_columns = [col for col in required_columns if col not in data.columns]
                    
                    if missing_columns:
                        issue = f"è‚¡ç¥¨æ•°æ®åˆ—ç¼ºå¤±: {code} - {missing_columns}"
                        self.validation_results['issues'].append(issue)
                        self.validation_results['passed'] = False
                        
                        # å°è¯•è‡ªåŠ¨ä¿®å¤ï¼šåˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
                        self._remove_corrupted_cache(code, period)
                        continue
                    
                    # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
                    if data.empty:
                        issue = f"è‚¡ç¥¨æ•°æ®ä¸ºç©º: {code}"
                        self.validation_results['issues'].append(issue)
                        self.validation_results['passed'] = False
                        self._remove_corrupted_cache(code, period)
                        continue
                    
                    # æ£€æŸ¥æ•°æ®è´¨é‡
                    self._validate_data_quality(data, code)
                    
                except Exception as e:
                    issue = f"è‚¡ç¥¨æ•°æ®è¯»å–å¤±è´¥: {code} - {str(e)}"
                    self.validation_results['issues'].append(issue)
                    self.validation_results['passed'] = False
                    self._remove_corrupted_cache(code, period)
                
            except Exception as e:
                logger.error(f"éªŒè¯è‚¡ç¥¨æ•°æ®æ—¶å‡ºé”™ {code}: {e}")
    
    def _validate_indicators_data(self, stock_codes: List[str]):
        """éªŒè¯æŠ€æœ¯æŒ‡æ ‡æ•°æ®"""
        logger.info("ğŸ“ˆ éªŒè¯æŠ€æœ¯æŒ‡æ ‡æ•°æ®...")
        
        for code in stock_codes:
            try:
                indicators_file = self.indicators_dir / f"{code}_indicators.csv"
                
                if indicators_file.exists():
                    try:
                        indicators = pd.read_csv(indicators_file, index_col=0, parse_dates=True)
                        
                        # æ£€æŸ¥å…³é”®æŠ€æœ¯æŒ‡æ ‡åˆ—
                        expected_indicators = ['ema_20w', 'ema_60w', 'rsi_14w', 'macd_dif', 'macd_dea', 'macd_hist']
                        missing_indicators = [ind for ind in expected_indicators if ind not in indicators.columns]
                        
                        if missing_indicators:
                            issue = f"æŠ€æœ¯æŒ‡æ ‡ç¼ºå¤±: {code} - {missing_indicators}"
                            self.validation_results['issues'].append(issue)
                            self.validation_results['passed'] = False
                            
                            # åˆ é™¤ä¸å®Œæ•´çš„æŒ‡æ ‡ç¼“å­˜
                            indicators_file.unlink()
                            self.validation_results['auto_fixed'].append(f"åˆ é™¤ä¸å®Œæ•´æŒ‡æ ‡ç¼“å­˜: {code}")
                        
                    except Exception as e:
                        issue = f"æŠ€æœ¯æŒ‡æ ‡æ•°æ®æŸå: {code} - {str(e)}"
                        self.validation_results['issues'].append(issue)
                        self.validation_results['passed'] = False
                        
                        # åˆ é™¤æŸåçš„æŒ‡æ ‡ç¼“å­˜
                        if indicators_file.exists():
                            indicators_file.unlink()
                            self.validation_results['auto_fixed'].append(f"åˆ é™¤æŸåæŒ‡æ ‡ç¼“å­˜: {code}")
                
            except Exception as e:
                logger.error(f"éªŒè¯æŠ€æœ¯æŒ‡æ ‡æ—¶å‡ºé”™ {code}: {e}")
    
    def _validate_data_format(self, stock_codes: List[str], period: str):
        """éªŒè¯æ•°æ®æ ¼å¼ä¸€è‡´æ€§"""
        logger.info("ğŸ” éªŒè¯æ•°æ®æ ¼å¼ä¸€è‡´æ€§...")
        
        # æ£€æŸ¥åˆ—åæ ¼å¼æ˜¯å¦ç»Ÿä¸€
        column_formats = {}
        
        for code in stock_codes:
            try:
                data_file = self.stock_data_dir / period / f"{code}.csv"
                if data_file.exists():
                    data = pd.read_csv(data_file, index_col=0, parse_dates=True, nrows=1)
                    columns_key = tuple(sorted(data.columns))
                    
                    if columns_key not in column_formats:
                        column_formats[columns_key] = []
                    column_formats[columns_key].append(code)
            
            except Exception as e:
                logger.warning(f"æ£€æŸ¥æ•°æ®æ ¼å¼æ—¶å‡ºé”™ {code}: {e}")
        
        # å¦‚æœå­˜åœ¨å¤šç§åˆ—æ ¼å¼ï¼Œæ ‡è®°ä¸ºé—®é¢˜
        if len(column_formats) > 1:
            issue = "æ•°æ®æ ¼å¼ä¸ä¸€è‡´ï¼Œå­˜åœ¨å¤šç§åˆ—åæ ¼å¼"
            self.validation_results['issues'].append(issue)
            self.validation_results['passed'] = False
            
            # æ‰¾å‡ºæœ€å¸¸è§çš„æ ¼å¼ä½œä¸ºæ ‡å‡†
            standard_format = max(column_formats.keys(), key=lambda k: len(column_formats[k]))
            
            for columns, codes in column_formats.items():
                if columns != standard_format:
                    for code in codes:
                        self._remove_corrupted_cache(code, period)
                        self.validation_results['auto_fixed'].append(f"åˆ é™¤æ ¼å¼ä¸ä¸€è‡´çš„ç¼“å­˜: {code}")
    
    def _validate_date_ranges(self, stock_codes: List[str], period: str):
        """éªŒè¯æ•°æ®æ—¶é—´èŒƒå›´"""
        logger.info("ğŸ“… éªŒè¯æ•°æ®æ—¶é—´èŒƒå›´...")
        
        current_date = datetime.now()
        
        for code in stock_codes:
            try:
                metadata_file = self.stock_data_dir / period / f"{code}.json"
                
                if metadata_file.exists():
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    
                    # æ£€æŸ¥æ•°æ®æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡7å¤©æœªæ›´æ–°ï¼‰
                    save_time = datetime.fromisoformat(metadata['save_time'])
                    age_days = (current_date - save_time).days
                    
                    if age_days > 7:
                        issue = f"æ•°æ®è¿‡æœŸ: {code} (å·² {age_days} å¤©æœªæ›´æ–°)"
                        self.validation_results['issues'].append(issue)
                        # æ³¨æ„ï¼šä¸è‡ªåŠ¨åˆ é™¤è¿‡æœŸæ•°æ®ï¼Œå› ä¸ºå¯èƒ½æ˜¯å‘¨æœ«æˆ–èŠ‚å‡æ—¥
                        self.validation_results['manual_action_required'].append(f"æ£€æŸ¥æ•°æ®æº: {code}")
            
            except Exception as e:
                logger.warning(f"æ£€æŸ¥æ•°æ®æ—¶é—´èŒƒå›´æ—¶å‡ºé”™ {code}: {e}")
    
    def _validate_data_quality(self, data: pd.DataFrame, code: str):
        """éªŒè¯æ•°æ®è´¨é‡"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        
        for col in numeric_columns:
            if col in data.columns:
                # æ£€æŸ¥æ˜¯å¦æœ‰NaNå€¼
                if data[col].isna().any():
                    issue = f"æ•°æ®åŒ…å«NaNå€¼: {code} - {col}åˆ—"
                    self.validation_results['issues'].append(issue)
                    self.validation_results['passed'] = False
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è´Ÿå€¼ï¼ˆé™¤äº†æŠ€æœ¯æŒ‡æ ‡ï¼‰
                if col != 'volume' and (data[col] < 0).any():
                    issue = f"æ•°æ®åŒ…å«è´Ÿå€¼: {code} - {col}åˆ—"
                    self.validation_results['issues'].append(issue)
                    self.validation_results['passed'] = False
                
                # æ£€æŸ¥ä»·æ ¼é€»è¾‘ï¼ˆhigh >= low, closeåœ¨highå’Œlowä¹‹é—´ï¼‰
                if col == 'high' and 'low' in data.columns:
                    if (data['high'] < data['low']).any():
                        issue = f"ä»·æ ¼é€»è¾‘é”™è¯¯: {code} - æœ€é«˜ä»·ä½äºæœ€ä½ä»·"
                        self.validation_results['issues'].append(issue)
                        self.validation_results['passed'] = False
    
    def _remove_corrupted_cache(self, code: str, period: str):
        """åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶"""
        try:
            data_file = self.stock_data_dir / period / f"{code}.csv"
            metadata_file = self.stock_data_dir / period / f"{code}.json"
            indicators_file = self.indicators_dir / f"{code}_indicators.csv"
            
            files_removed = []
            for file_path in [data_file, metadata_file, indicators_file]:
                if file_path.exists():
                    file_path.unlink()
                    files_removed.append(str(file_path))
            
            if files_removed:
                self.validation_results['auto_fixed'].append(f"åˆ é™¤æŸåç¼“å­˜: {code} - {len(files_removed)}ä¸ªæ–‡ä»¶")
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶: {code}")
        
        except Exception as e:
            logger.error(f"åˆ é™¤æŸåç¼“å­˜æ—¶å‡ºé”™ {code}: {e}")
    
    def _generate_validation_report(self) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report = self.validation_results.copy()
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        report['summary'] = {
            'total_issues': len(report['issues']),
            'auto_fixed_count': len(report['auto_fixed']),
            'manual_action_count': len(report['manual_action_required']),
            'validation_time': datetime.now().isoformat()
        }
        
        # è®°å½•æ—¥å¿—
        if report['passed']:
            logger.info("âœ… ç¼“å­˜æ•°æ®éªŒè¯é€šè¿‡")
        else:
            logger.warning(f"âš ï¸ å‘ç° {report['summary']['total_issues']} ä¸ªé—®é¢˜")
            logger.info(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤ {report['summary']['auto_fixed_count']} ä¸ªé—®é¢˜")
            
            if report['summary']['manual_action_count'] > 0:
                logger.warning(f"âš¡ éœ€è¦æ‰‹åŠ¨å¤„ç† {report['summary']['manual_action_count']} ä¸ªé—®é¢˜")
        
        return report
    
    def get_cache_health_status(self) -> str:
        """è·å–ç¼“å­˜å¥åº·çŠ¶æ€"""
        if self.validation_results['passed']:
            return "HEALTHY"
        elif len(self.validation_results['manual_action_required']) == 0:
            return "AUTO_FIXED"
        else:
            return "NEEDS_ATTENTION"


def validate_cache_before_backtest(stock_codes: List[str], period: str = 'weekly') -> bool:
    """
    å›æµ‹å‰çš„ç¼“å­˜éªŒè¯å…¥å£å‡½æ•°
    
    Args:
        stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        period: æ•°æ®å‘¨æœŸ
        
    Returns:
        bool: æ˜¯å¦å¯ä»¥ç»§ç»­å›æµ‹
    """
    logger.info("ğŸ” æ‰§è¡Œå›æµ‹å‰ç¼“å­˜éªŒè¯...")
    
    validator = CacheValidator()
    result = validator.validate_and_fix(stock_codes, period)
    
    status = validator.get_cache_health_status()
    
    if status == "HEALTHY":
        logger.info("âœ… ç¼“å­˜çŠ¶æ€è‰¯å¥½ï¼Œå¯ä»¥ç»§ç»­å›æµ‹")
        return True
    elif status == "AUTO_FIXED":
        logger.info("ğŸ”§ ç¼“å­˜é—®é¢˜å·²è‡ªåŠ¨ä¿®å¤ï¼Œå¯ä»¥ç»§ç»­å›æµ‹")
        return True
    else:
        logger.error("âŒ ç¼“å­˜å­˜åœ¨éœ€è¦æ‰‹åŠ¨å¤„ç†çš„é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥åå†å›æµ‹")
        logger.error("ğŸ’¡ å»ºè®®æ‰‹åŠ¨åˆ é™¤ data_cache/ ç›®å½•åé‡æ–°è¿è¡Œå›æµ‹")
        return False


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # æµ‹è¯•éªŒè¯åŠŸèƒ½
    test_codes = ['601088', '601225', '600985']
    result = validate_cache_before_backtest(test_codes)
    print(f"éªŒè¯ç»“æœ: {'é€šè¿‡' if result else 'å¤±è´¥'}")
