"""
ç”³ä¸‡äºŒçº§è¡Œä¸šRSIé˜ˆå€¼è®¡ç®—æ¨¡å—
æ ¹æ®è¡Œä¸šæ³¢åŠ¨ç‡åˆ†å±‚ï¼ŒåŠ¨æ€è®¡ç®—RSIè¶…ä¹°è¶…å–é˜ˆå€¼
"""

import logging
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Tuple

import akshare as ak
import numpy as np
import pandas as pd

# å¯¼å…¥é…ç½®æ–‡ä»¶
try:
    from .config import (
        CALCULATION_PERIODS,
        DATA_QUALITY,
        EXTREME_THRESHOLD_COEFFICIENTS,
        OUTPUT_CONFIG,
        RSI_THRESHOLDS,
    )
except ImportError:
    # å¦‚æœä½œä¸ºè„šæœ¬ç›´æ¥è¿è¡Œï¼Œä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    from config import (
        CALCULATION_PERIODS,
        DATA_QUALITY,
        EXTREME_THRESHOLD_COEFFICIENTS,
        OUTPUT_CONFIG,
        RSI_THRESHOLDS,
    )

try:
    import talib
    USE_TALIB = True
except ImportError:
    USE_TALIB = False
    print("Warning: talib not available, using custom RSI implementation")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def calculate_rsi(prices: pd.Series, period: int = 14) -> pd.Series:
    """
    è®¡ç®—RSIæŒ‡æ ‡
    
    Args:
        prices: ä»·æ ¼åºåˆ—
        period: RSIå‘¨æœŸ
        
    Returns:
        RSIåºåˆ—
    """
    if USE_TALIB:
        return pd.Series(talib.RSI(prices.values, timeperiod=period), index=prices.index)
    else:
        # è‡ªå®šä¹‰RSIè®¡ç®—
        delta = prices.diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        
        avg_gain = gain.rolling(window=period).mean()
        avg_loss = loss.rolling(window=period).mean()
        
        rs = avg_gain / avg_loss
        rsi = 100 - (100 / (1 + rs))
        
        return rsi


class SWIndustryRSIThresholds:
    """ç”³ä¸‡äºŒçº§è¡Œä¸šRSIé˜ˆå€¼è®¡ç®—å™¨"""
    
    def __init__(self, output_dir: str = "output"):
        """
        åˆå§‹åŒ–
        
        Args:
            output_dir: è¾“å‡ºæ–‡ä»¶ç›®å½•
        """
        self.output_dir = output_dir
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
        self.rsi_period = CALCULATION_PERIODS["rsi_period"]
        self.lookback_weeks = CALCULATION_PERIODS["lookback_weeks"]
        self.retry_times = DATA_QUALITY["retry_times"]
        self.retry_delay = DATA_QUALITY["retry_delay"]
        self.min_data_points = DATA_QUALITY["min_data_points"]
        self.min_rsi_points = DATA_QUALITY["min_rsi_points"]
        
        # RSIé˜ˆå€¼é…ç½®
        self.rsi_thresholds = RSI_THRESHOLDS
        self.volatility_quantiles = CALCULATION_PERIODS["volatility_quantiles"]
        self.extreme_threshold_coefficients = EXTREME_THRESHOLD_COEFFICIENTS
        
        # è¾“å‡ºé…ç½®
        self.output_config = OUTPUT_CONFIG
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # æ‰“å°å½“å‰é…ç½®
        logger.info("RSIé˜ˆå€¼è®¡ç®—é…ç½®:")
        logger.info(f"  å†å²æ•°æ®å‘¨æ•°: {self.lookback_weeks}")
        logger.info(f"  RSIè®¡ç®—å‘¨æœŸ: {self.rsi_period}")
        logger.info(f"  æ™®é€šé˜ˆå€¼: è¶…å–{self.rsi_thresholds['æ™®é€šè¶…å–']}%, è¶…ä¹°{self.rsi_thresholds['æ™®é€šè¶…ä¹°']}%")
        logger.info(f"  æ³¢åŠ¨ç‡åˆ†å±‚: Q1={self.volatility_quantiles['q1']}%, Q3={self.volatility_quantiles['q3']}%")
    
    def get_sw_industry_codes(self) -> pd.DataFrame:
        """
        è·å–ç”³ä¸‡äºŒçº§è¡Œä¸šä»£ç å’Œåç§°
        
        Returns:
            åŒ…å«è¡Œä¸šä»£ç å’Œåç§°çš„DataFrame
        """
        try:
            logger.info("ä½¿ç”¨AkShare APIè·å–ç”³ä¸‡äºŒçº§è¡Œä¸šåˆ†ç±»...")
            
            # ä½¿ç”¨AkShareçš„sw_index_second_infoæ¥å£
            sw_industry = ak.sw_index_second_info()
            
            if sw_industry.empty:
                raise ValueError("AkShare APIè¿”å›ç©ºæ•°æ®")
            
            logger.debug(f"APIè¿”å›æ•°æ®åˆ—å: {list(sw_industry.columns)}")
            logger.debug(f"APIè¿”å›æ•°æ®æ ·ä¾‹:\n{sw_industry.head()}")
            
            # å¤„ç†è¿”å›çš„æ•°æ®æ ¼å¼
            if 'è¡Œä¸šä»£ç ' in sw_industry.columns and 'è¡Œä¸šåç§°' in sw_industry.columns:
                # æå–éœ€è¦çš„åˆ—
                df = sw_industry[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°']].copy()
                
                # å¤„ç†è¡Œä¸šä»£ç æ ¼å¼ï¼ˆå»æ‰å¯èƒ½çš„.SIåç¼€ï¼‰
                df['è¡Œä¸šä»£ç '] = df['è¡Œä¸šä»£ç '].astype(str)
                df['è¡Œä¸šä»£ç '] = df['è¡Œä¸šä»£ç '].str.replace('.SI', '', regex=False)
                
                # é‡å‘½ååˆ—ä»¥ä¿æŒä¸€è‡´æ€§
                df = df.rename(columns={'è¡Œä¸šä»£ç ': 'æŒ‡æ•°ä»£ç ', 'è¡Œä¸šåç§°': 'æŒ‡æ•°åç§°'})
                
                # å»é‡å¹¶æ’åº
                df = df.drop_duplicates().sort_values('æŒ‡æ•°ä»£ç ').reset_index(drop=True)
                
                logger.info(f"é€šè¿‡AkShare APIè·å–åˆ° {len(df)} ä¸ªç”³ä¸‡äºŒçº§è¡Œä¸š")
                logger.info(f"è¡Œä¸šä»£ç æ ¼å¼ç¤ºä¾‹: {df['æŒ‡æ•°ä»£ç '].head(3).tolist()}")
                
                return df
            else:
                raise ValueError(f"APIè¿”å›æ•°æ®æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œåˆ—å: {list(sw_industry.columns)}")
                
        except Exception as e:
            logger.error(f"AkShare APIè·å–ç”³ä¸‡è¡Œä¸šåˆ†ç±»å¤±è´¥: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨é¢„å®šä¹‰åˆ—è¡¨
            logger.info("ä½¿ç”¨é¢„å®šä¹‰çš„ç”³ä¸‡äºŒçº§è¡Œä¸šåˆ—è¡¨...")
            fallback_codes = {
                    '620100': 'æˆ¿å±‹å»ºè®¾', '480400': 'åŸå•†è¡Œ', '480500': 'å†œå•†è¡Œ', '480300': 'è‚¡ä»½åˆ¶é“¶è¡Œ',
                    '480200': 'å›½æœ‰å¤§å‹é“¶è¡Œ', '490200': 'ä¿é™©', '421100': 'èˆªè¿æ¸¯å£', '750100': 'æ²¹æ°”å¼€é‡‡',
                    '620300': 'åŸºç¡€å»ºè®¾', '330100': 'ç™½è‰²å®¶ç”µ', '740100': 'ç…¤ç‚­å¼€é‡‡', '110700': 'å…»æ®–ä¸š',
                    '750300': 'ç‚¼åŒ–åŠè´¸æ˜“', '240300': 'å·¥ä¸šé‡‘å±', '420900': 'é“è·¯å…¬è·¯', '340500': 'ç™½é…’',
                    '410100': 'ç”µåŠ›', '730100': 'é€šä¿¡æœåŠ¡', '620400': 'ä¸“ä¸šå·¥ç¨‹', '420800': 'ç‰©æµ',
                    '410300': 'ç‡ƒæ°”', '750200': 'æ²¹æœå·¥ç¨‹', '640500': 'è½¨äº¤è®¾å¤‡', '350100': 'çººç»‡åˆ¶é€ ',
                    '450200': 'è´¸æ˜“', '490100': 'è¯åˆ¸', '370400': 'åŒ»è¯å•†ä¸š', '330400': 'å¨å«ç”µå™¨',
                    '720900': 'å‡ºç‰ˆ', '640600': 'å·¥ç¨‹æœºæ¢°', '110400': 'é¥²æ–™', '230500': 'ç‰¹é’¢',
                    '220200': 'åŒ–å­¦åŸæ–™', '330300': 'å°å®¶ç”µ', '340400': 'é£Ÿå“åŠ å·¥', '490300': 'å¤šå…ƒé‡‘è',
                    '280500': 'ä¹˜ç”¨è½¦', '330200': 'é»‘è‰²å®¶ç”µ', '220800': 'å†œåŒ–åˆ¶å“', '340600': 'éç™½é…’',
                    '280400': 'æ‘©æ‰˜è½¦åŠå…¶ä»–', '350300': 'é¥°å“', '230300': 'å†¶é’¢åŸæ–™', '280200': 'æ±½è½¦é›¶éƒ¨ä»¶',
                    '240400': 'è´µé‡‘å±', '340900': 'è°ƒå‘³å‘é…µå“', '360300': 'å®¶å±…ç”¨å“', '340700': 'é¥®æ–™ä¹³å“',
                    '610100': 'æ°´æ³¥', '760100': 'ç¯å¢ƒæ²»ç†', '220300': 'åŒ–å­¦åˆ¶å“', '630800': 'ç”µç½‘è®¾å¤‡',
                    '370200': 'ä¸­è¯', '330500': 'ç…§æ˜è®¾å¤‡', '450700': 'æ—…æ¸¸é›¶å”®', '460800': 'ä¸“ä¸šæœåŠ¡',
                    '270500': 'æ¶ˆè´¹ç”µå­', '330600': 'å®¶ç”µé›¶éƒ¨ä»¶', '630700': 'ç”µæ± ', '340800': 'ä¼‘é—²é£Ÿå“',
                    '240500': 'å°é‡‘å±', '770100': 'ä¸ªæŠ¤ç”¨å“', '370600': 'åŒ»ç–—æœåŠ¡', '610300': 'è£…ä¿®å»ºæ',
                    '280600': 'å•†ç”¨è½¦', '370500': 'åŒ»ç–—å™¨æ¢°', '360200': 'åŒ…è£…å°åˆ·', '460900': 'é…’åº—é¤é¥®',
                    '620600': 'å·¥ç¨‹å’¨è¯¢æœåŠ¡', '770200': 'åŒ–å¦†å“', '220600': 'æ©¡èƒ¶', '710100': 'è®¡ç®—æœºè®¾å¤‡',
                    '220400': 'åŒ–å­¦çº¤ç»´', '360500': 'æ–‡å¨±ç”¨å“', '330700': 'å…¶ä»–å®¶ç”µ', '770300': 'åŒ»ç–—ç¾å®¹',
                    '270200': 'å…ƒä»¶', '630600': 'é£ç”µè®¾å¤‡', '610200': 'ç»ç’ƒç»çº¤', '650400': 'èˆªæµ·è£…å¤‡',
                    '640200': 'ä¸“ç”¨è®¾å¤‡', '240200': 'é‡‘å±æ–°ææ–™', '630300': 'å…¶ä»–ç”µæºè®¾å¤‡', '110900': 'å†œä¸šç»¼åˆ',
                    '730200': 'é€šä¿¡è®¾å¤‡', '110500': 'å†œäº§å“åŠ å·¥', '450600': 'äº’è”ç½‘ç”µå•†', '110100': 'ç§æ¤ä¸š',
                    '640100': 'é€šç”¨è®¾å¤‡', '630100': 'ç”µæœº', '270400': 'å…¶ä»–ç”µå­', '450400': 'ä¸“ä¸šè¿é”',
                    '270600': 'ç”µå­åŒ–å­¦å“', '220900': 'éé‡‘å±ææ–™', '220500': 'å¡‘æ–™', '640700': 'è‡ªåŠ¨åŒ–è®¾å¤‡',
                    '370300': 'ç”Ÿç‰©åˆ¶å“', '760200': 'ç¯ä¿è®¾å¤‡', '240600': 'èƒ½æºé‡‘å±', '720700': 'æ•°å­—åª’ä½“',
                    '110800': 'åŠ¨ç‰©ä¿å¥', '350200': 'æœè£…å®¶çºº', '650200': 'èˆªç©ºè£…å¤‡', '370100': 'åŒ–å­¦åˆ¶è¯',
                    '430300': 'æˆ¿åœ°äº§æœåŠ¡', '461000': 'æ—…æ¸¸åŠæ™¯åŒº', '720400': 'æ¸¸æˆ', '270300': 'å…‰å­¦å…‰ç”µå­',
                    '450300': 'ä¸€èˆ¬é›¶å”®', '270100': 'åŠå¯¼ä½“', '720500': 'å¹¿å‘Šè¥é”€', '280300': 'æ±½è½¦æœåŠ¡',
                    '650100': 'èˆªå¤©è£…å¤‡', '710400': 'è½¯ä»¶å¼€å‘', '650500': 'å†›å·¥ç”µå­', '710300': 'ITæœåŠ¡',
                    '421000': 'èˆªç©ºæœºåœº', '510100': 'ç»¼åˆ', '650300': 'åœ°é¢å…µè£…', '720600': 'å½±è§†é™¢çº¿',
                    '461100': 'æ•™è‚²', '460600': 'ä½“è‚²', '721000': 'ç”µè§†å¹¿æ’­', '110300': 'æ—ä¸š',
                    '360100': 'é€ çº¸', '230400': 'æ™®é’¢', '630500': 'å…‰ä¼è®¾å¤‡', '110200': 'æ¸”ä¸š',
                    '620200': 'è£…ä¿®è£…é¥°', '740200': 'ç„¦ç‚­', '430100': 'æˆ¿åœ°äº§å¼€å‘'
            }
            
            df = pd.DataFrame(list(fallback_codes.items()), 
                            columns=['æŒ‡æ•°ä»£ç ', 'æŒ‡æ•°åç§°'])
            logger.info(f"ä½¿ç”¨é¢„å®šä¹‰è¡Œä¸šä»£ç ï¼Œå…± {len(df)} ä¸ªè¡Œä¸š")
            return df
    
    def get_industry_weekly_data_with_retry(self, code: str, weeks: int = None) -> pd.DataFrame:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®è·å–
        
        Args:
            code: è¡Œä¸šä»£ç 
            weeks: å›çœ‹å‘¨æ•°
            
        Returns:
            åŒ…å«å‘¨çº¿æ•°æ®çš„DataFrame
        """
        for attempt in range(self.retry_times):
            try:
                df = self.get_industry_weekly_data(code, weeks)
                if not df.empty:
                    return df
            except Exception as e:
                logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•è·å– {code} æ•°æ®å¤±è´¥: {e}")
                if attempt < self.retry_times - 1:
                    time.sleep(self.retry_delay)
        
        logger.error(f"è·å– {code} æ•°æ®å¤±è´¥ï¼Œå·²é‡è¯• {self.retry_times} æ¬¡")
        return pd.DataFrame()
    
    def get_industry_weekly_data(self, code: str, weeks: int = None) -> pd.DataFrame:
        """
        è·å–å•ä¸ªè¡Œä¸šçš„å‘¨çº¿æ•°æ®
        
        Args:
            code: è¡Œä¸šä»£ç 
            weeks: å›çœ‹å‘¨æ•°ï¼Œé»˜è®¤ä½¿ç”¨self.lookback_weeks
            
        Returns:
            åŒ…å«å‘¨çº¿æ•°æ®çš„DataFrame
        """
        if weeks is None:
            weeks = self.lookback_weeks
            
        try:
            logger.debug(f"è·å– {code} çš„å‘¨çº¿æ•°æ®")
            
            # ä½¿ç”¨æ­£ç¡®çš„AkShare APIè·å–ç”³ä¸‡è¡Œä¸šæŒ‡æ•°å‘¨çº¿æ•°æ®
            df = ak.index_hist_sw(symbol=code, period="week")
            
            if df.empty:
                logger.warning(f"è¡Œä¸š {code} æ•°æ®ä¸ºç©º")
                return pd.DataFrame()
            
            logger.debug(f"è·å–åˆ° {code} æ•°æ®ï¼Œå…± {len(df)} è¡Œï¼Œåˆ—å: {list(df.columns)}")
            
            # æ•°æ®é¢„å¤„ç† - å¤„ç†APIè¿”å›çš„æ•°æ®æ ¼å¼
            # index_hist_swè¿”å›çš„åˆ—å: ['ä»£ç ', 'æ—¥æœŸ', 'æ”¶ç›˜', 'å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢']
            if 'æ—¥æœŸ' not in df.columns or 'æ”¶ç›˜' not in df.columns:
                logger.error(f"è¡Œä¸š {code} æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œåˆ—å: {list(df.columns)}")
                return pd.DataFrame()
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼å¹¶è®¾ç½®ä¸ºç´¢å¼•
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
            df.set_index('æ—¥æœŸ', inplace=True)
            df.sort_index(inplace=True)
            
            # ç¡®ä¿æ”¶ç›˜ä»·ä¸ºæ•°å€¼ç±»å‹
            df['æ”¶ç›˜'] = pd.to_numeric(df['æ”¶ç›˜'], errors='coerce')
            
            # å»é™¤æ”¶ç›˜ä»·ä¸ºç©ºçš„è¡Œ
            df = df.dropna(subset=['æ”¶ç›˜'])
            
            if len(df) == 0:
                logger.warning(f"è¡Œä¸š {code} æ”¶ç›˜ä»·æ•°æ®å…¨éƒ¨ä¸ºç©º")
                return pd.DataFrame()
            
            # è®¡ç®—RSI
            df['rsi14'] = calculate_rsi(df['æ”¶ç›˜'], period=self.rsi_period)
            
            # åªä¿ç•™æœ‰RSIæ•°æ®çš„è¡Œ
            df = df.dropna(subset=['rsi14'])
            
            if len(df) < weeks:
                logger.warning(f"è¡Œä¸š {code} æ•°æ®ä¸è¶³ï¼Œåªæœ‰ {len(df)} å‘¨ï¼Œéœ€è¦ {weeks} å‘¨")
            
            # åªä¿ç•™æœ€è¿‘çš„æŒ‡å®šå‘¨æ•°
            result_df = df.tail(weeks) if len(df) >= weeks else df
            
            logger.debug(f"è¡Œä¸š {code} æœ€ç»ˆè¿”å› {len(result_df)} å‘¨æ•°æ®")
            return result_df
            
        except Exception as e:
            logger.error(f"è·å–è¡Œä¸š {code} æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def calculate_volatility_layers(self, sigma_list: List[float]) -> Tuple[float, float]:
        """
        è®¡ç®—æ³¢åŠ¨ç‡åˆ†å±‚çš„åˆ†ä½ç‚¹
        
        Args:
            sigma_list: æ‰€æœ‰è¡Œä¸šçš„æ³¢åŠ¨ç‡åˆ—è¡¨
            
        Returns:
            (q1, q3): åˆ†ä½ç‚¹
        """
        q1_pct = self.volatility_quantiles['q1']
        q3_pct = self.volatility_quantiles['q3']
        
        q1 = np.percentile(sigma_list, q1_pct)
        q3 = np.percentile(sigma_list, q3_pct)
        
        logger.info(f"æ³¢åŠ¨ç‡åˆ†å±‚: Q{q1_pct}={q1:.3f}, Q{q3_pct}={q3:.3f}")
        return q1, q3
    
    def get_layer_percentiles(self, sigma: float, q1: float, q3: float) -> Tuple[str, int, int]:
        """
        æ ¹æ®æ³¢åŠ¨ç‡ç¡®å®šåˆ†å±‚å’Œå¯¹åº”çš„æç«¯åˆ†ä½æ•°
        
        Args:
            sigma: è¡Œä¸šæ³¢åŠ¨ç‡
            q1: ä½åˆ†ä½ç‚¹
            q3: é«˜åˆ†ä½ç‚¹
            
        Returns:
            (layer, pct_low, pct_high): åˆ†å±‚åç§°å’Œæç«¯åˆ†ä½æ•°
        """
        extreme_thresholds = self.rsi_thresholds["æç«¯é˜ˆå€¼"]
        
        if sigma >= q3:
            layer = 'é«˜æ³¢åŠ¨'
            pct_low = extreme_thresholds["é«˜æ³¢åŠ¨"]["è¶…å–"]
            pct_high = extreme_thresholds["é«˜æ³¢åŠ¨"]["è¶…ä¹°"]
        elif sigma < q1:
            layer = 'ä½æ³¢åŠ¨'
            pct_low = extreme_thresholds["ä½æ³¢åŠ¨"]["è¶…å–"]
            pct_high = extreme_thresholds["ä½æ³¢åŠ¨"]["è¶…ä¹°"]
        else:
            layer = 'ä¸­æ³¢åŠ¨'
            pct_low = extreme_thresholds["ä¸­æ³¢åŠ¨"]["è¶…å–"]
            pct_high = extreme_thresholds["ä¸­æ³¢åŠ¨"]["è¶…ä¹°"]
            
        return layer, pct_low, pct_high
    
    def calculate_single_industry_thresholds(self, rsi_series: pd.Series, 
                                           sigma: float, q1: float, q3: float) -> Dict:
        """
        è®¡ç®—å•ä¸ªè¡Œä¸šçš„RSIé˜ˆå€¼
        
        Args:
            rsi_series: RSIæ—¶é—´åºåˆ—
            sigma: è¡Œä¸šæ³¢åŠ¨ç‡
            q1: ä½åˆ†ä½ç‚¹
            q3: é«˜åˆ†ä½ç‚¹
            
        Returns:
            åŒ…å«å„ç§é˜ˆå€¼çš„å­—å…¸
        """
        layer, pct_low, pct_high = self.get_layer_percentiles(sigma, q1, q3)
        
        # ä»é…ç½®æ–‡ä»¶è·å–æ™®é€šé˜ˆå€¼åˆ†ä½æ•°
        normal_oversold = self.rsi_thresholds['æ™®é€šè¶…å–']
        normal_overbought = self.rsi_thresholds['æ™®é€šè¶…ä¹°']
        
        # è®¡ç®—åŸå§‹æç«¯é˜ˆå€¼
        raw_extreme_oversold = float(np.percentile(rsi_series, pct_low))
        raw_extreme_overbought = float(np.percentile(rsi_series, pct_high))
        
        # è·å–å¯¹åº”æ³¢åŠ¨åˆ†å±‚çš„ç³»æ•°
        layer_coefficients = self.extreme_threshold_coefficients.get(layer, {
            "è¶…å–ç³»æ•°": 1.0,
            "è¶…ä¹°ç³»æ•°": 1.0
        })
        oversold_coefficient = layer_coefficients["è¶…å–ç³»æ•°"]
        overbought_coefficient = layer_coefficients["è¶…ä¹°ç³»æ•°"]
        
        # è®¡ç®—è°ƒæ•´åçš„æç«¯é˜ˆå€¼
        adjusted_extreme_oversold = raw_extreme_oversold * oversold_coefficient
        adjusted_extreme_overbought = raw_extreme_overbought * overbought_coefficient
        
        # è®¡ç®—å„ç§é˜ˆå€¼
        thresholds = {
            'layer': layer,
            'volatility': float(sigma),
            'current_rsi': float(rsi_series.iloc[-1]) if len(rsi_series) > 0 else np.nan,
            'æ™®é€šè¶…å–': float(np.percentile(rsi_series, normal_oversold)),
            'æ™®é€šè¶…ä¹°': float(np.percentile(rsi_series, normal_overbought)),
            'æç«¯è¶…å–': adjusted_extreme_oversold,
            'æç«¯è¶…ä¹°': adjusted_extreme_overbought,
            'data_points': len(rsi_series)
        }
        
        logger.debug(f"è¡Œä¸š {layer} é˜ˆå€¼è®¡ç®—å®Œæˆï¼Œç³»æ•°åº”ç”¨å¦‚ä¸‹:")
        logger.debug(f"  åŸå§‹æç«¯è¶…å–: {raw_extreme_oversold:.3f}, ç³»æ•°: {oversold_coefficient}, è°ƒæ•´å: {adjusted_extreme_oversold:.3f}")
        logger.debug(f"  åŸå§‹æç«¯è¶…ä¹°: {raw_extreme_overbought:.3f}, ç³»æ•°: {overbought_coefficient}, è°ƒæ•´å: {adjusted_extreme_overbought:.3f}")
        
        return thresholds
    
    def calculate_all_thresholds(self) -> pd.DataFrame:
        """
        è®¡ç®—æ‰€æœ‰ç”³ä¸‡äºŒçº§è¡Œä¸šçš„RSIé˜ˆå€¼
        
        Returns:
            åŒ…å«æ‰€æœ‰è¡Œä¸šé˜ˆå€¼çš„DataFrame
        """
        logger.info("å¼€å§‹è®¡ç®—ç”³ä¸‡äºŒçº§è¡Œä¸šRSIé˜ˆå€¼...")
        
        # è·å–è¡Œä¸šä»£ç 
        industry_df = self.get_sw_industry_codes()
        
        # å­˜å‚¨æ‰€æœ‰è¡Œä¸šæ•°æ®å’Œæ³¢åŠ¨ç‡
        all_data = {}
        sigma_list = []
        
        # ç¬¬ä¸€è½®ï¼šè·å–æ‰€æœ‰è¡Œä¸šæ•°æ®å¹¶è®¡ç®—æ³¢åŠ¨ç‡
        total_industries = len(industry_df)
        logger.info(f"ç¬¬ä¸€è½®ï¼šè·å–è¡Œä¸šæ•°æ®å¹¶è®¡ç®—æ³¢åŠ¨ç‡...ï¼ˆå…±{total_industries}ä¸ªè¡Œä¸šï¼‰")
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ç¬¬ä¸€è½®ï¼šè·å–è¡Œä¸šæ•°æ®å¹¶è®¡ç®—æ³¢åŠ¨ç‡")
        print(f"æ€»è¡Œä¸šæ•°ï¼š{total_industries}")
        print(f"{'='*60}\n")
        
        start_time = time.time()
        for idx, row in industry_df.iterrows():
            code = row['æŒ‡æ•°ä»£ç ']
            name = row['æŒ‡æ•°åç§°']
            
            # è®¡ç®—è¿›åº¦
            current = idx + 1
            progress = (current / total_industries) * 100
            elapsed = time.time() - start_time
            avg_time = elapsed / current if current > 0 else 0
            remaining = avg_time * (total_industries - current)
            
            # æ˜¾ç¤ºè¿›åº¦æ¡
            bar_length = 40
            filled = int(bar_length * current / total_industries)
            bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
            
            print(f"\r[{bar}] {progress:5.1f}% | {current}/{total_industries} | "
                  f"å½“å‰: {code[:10]} {name[:12]} | "
                  f"å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ", end='', flush=True)
            
            logger.debug(f"å¤„ç†è¡Œä¸š: {code} - {name}")
            
            # è·å–å‘¨çº¿æ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰
            df = self.get_industry_weekly_data_with_retry(code)
            
            if df.empty or len(df) < self.min_data_points:
                logger.warning(f"è·³è¿‡è¡Œä¸š {code}ï¼Œæ•°æ®ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘{self.min_data_points}å‘¨ï¼‰")
                continue
            
            # è®¡ç®—æ³¢åŠ¨ç‡
            rsi_series = df['rsi14'].dropna()
            if len(rsi_series) < self.min_rsi_points:
                logger.warning(f"è·³è¿‡è¡Œä¸š {code}ï¼ŒRSIæ•°æ®ä¸è¶³ï¼ˆéœ€è¦è‡³å°‘{self.min_rsi_points}ä¸ªæ•°æ®ç‚¹ï¼‰")
                continue
                
            sigma = rsi_series.std()
            
            all_data[code] = {
                'name': name,
                'rsi_series': rsi_series,
                'sigma': sigma
            }
            sigma_list.append(sigma)
        
        if not sigma_list:
            raise ValueError("æ²¡æœ‰è·å–åˆ°æœ‰æ•ˆçš„è¡Œä¸šæ•°æ®")
        
        print(f"\n\nâœ… ç¬¬ä¸€è½®å®Œæˆï¼šæˆåŠŸè·å– {len(sigma_list)} ä¸ªè¡Œä¸šçš„æ•°æ®")
        print(f"â±ï¸  è€—æ—¶: {(time.time() - start_time)/60:.1f} åˆ†é’Ÿ\n")
        logger.info(f"æˆåŠŸè·å– {len(sigma_list)} ä¸ªè¡Œä¸šçš„æ•°æ®")
        
        # è®¡ç®—æ³¢åŠ¨ç‡åˆ†å±‚
        q1, q3 = self.calculate_volatility_layers(sigma_list)
        
        # ç¬¬äºŒè½®ï¼šè®¡ç®—å„è¡Œä¸šé˜ˆå€¼
        total_calc = len(all_data)
        logger.info(f"ç¬¬äºŒè½®ï¼šè®¡ç®—å„è¡Œä¸šRSIé˜ˆå€¼...ï¼ˆå…±{total_calc}ä¸ªè¡Œä¸šï¼‰")
        print(f"\n{'='*60}")
        print(f"ğŸ“ˆ ç¬¬äºŒè½®ï¼šè®¡ç®—å„è¡Œä¸šRSIé˜ˆå€¼")
        print(f"å¾…è®¡ç®—è¡Œä¸šæ•°ï¼š{total_calc}")
        print(f"{'='*60}\n")
        
        results = {}
        calc_start = time.time()
        
        for idx, (code, data) in enumerate(all_data.items()):
            name = data['name']
            rsi_series = data['rsi_series']
            sigma = data['sigma']
            
            # è®¡ç®—è¿›åº¦
            current = idx + 1
            progress = (current / total_calc) * 100
            
            # æ˜¾ç¤ºè¿›åº¦æ¡
            bar_length = 40
            filled = int(bar_length * current / total_calc)
            bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
            
            print(f"\r[{bar}] {progress:5.1f}% | {current}/{total_calc} | "
                  f"è®¡ç®—: {code[:10]} {name[:12]}", end='', flush=True)
            
            logger.debug(f"è®¡ç®— {code} - {name} çš„é˜ˆå€¼")
            
            # è®¡ç®—é˜ˆå€¼
            thresholds = self.calculate_single_industry_thresholds(
                rsi_series, sigma, q1, q3
            )
            thresholds['è¡Œä¸šåç§°'] = name
            
            results[code] = thresholds
        
        # è½¬æ¢ä¸ºDataFrame
        result_df = pd.DataFrame.from_dict(results, orient='index')
        result_df.index.name = 'è¡Œä¸šä»£ç '
        
        # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
        columns_order = [
            'è¡Œä¸šåç§°', 'layer', 'volatility', 'current_rsi',
            'æ™®é€šè¶…å–', 'æ™®é€šè¶…ä¹°', 'æç«¯è¶…å–', 'æç«¯è¶…ä¹°', 'data_points'
        ]
        result_df = result_df[columns_order]
        
        print(f"\n\nâœ… ç¬¬äºŒè½®å®Œæˆï¼šæˆåŠŸè®¡ç®— {len(result_df)} ä¸ªè¡Œä¸šçš„RSIé˜ˆå€¼")
        print(f"â±ï¸  è€—æ—¶: {(time.time() - calc_start):.1f} ç§’")
        print(f"\n{'='*60}")
        print(f"ğŸ‰ å…¨éƒ¨è®¡ç®—å®Œæˆï¼")
        print(f"æ€»è€—æ—¶: {(time.time() - start_time)/60:.1f} åˆ†é’Ÿ")
        print(f"{'='*60}\n")
        logger.info(f"æˆåŠŸè®¡ç®— {len(result_df)} ä¸ªè¡Œä¸šçš„RSIé˜ˆå€¼")
        return result_df
    
    def save_thresholds(self, df: pd.DataFrame, filename: str = None) -> str:
        """
        ä¿å­˜é˜ˆå€¼åˆ°CSVæ–‡ä»¶
        
        Args:
            df: é˜ˆå€¼DataFrame
            filename: æ–‡ä»¶åï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„åç§°
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if filename is None:
            filename = self.output_config['output_filename']
        
        filepath = os.path.join(self.output_dir, filename)
        
        # æ·»åŠ æ›´æ–°æ—¶é—´åˆ—å’Œé…ç½®ä¿¡æ¯
        df_copy = df.copy()
        df_copy['æ›´æ–°æ—¶é—´'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        if self.output_config['include_debug_info']:
            # æ·»åŠ é…ç½®ä¿¡æ¯ä½œä¸ºæ³¨é‡Šï¼ˆåœ¨æ–‡ä»¶å¼€å¤´ï¼‰
            config_info = [
                f"# é…ç½®å‚æ•°: å†å²å‘¨æ•°={self.lookback_weeks}, RSIå‘¨æœŸ={self.rsi_period}",
                f"# æ™®é€šé˜ˆå€¼: è¶…å–{self.rsi_thresholds['æ™®é€šè¶…å–']}%, è¶…ä¹°{self.rsi_thresholds['æ™®é€šè¶…ä¹°']}%",
                f"# æç«¯é˜ˆå€¼: é«˜æ³¢åŠ¨({self.rsi_thresholds['æç«¯é˜ˆå€¼']['é«˜æ³¢åŠ¨']['è¶…å–']}%,{self.rsi_thresholds['æç«¯é˜ˆå€¼']['é«˜æ³¢åŠ¨']['è¶…ä¹°']}%), " +
                f"ä¸­æ³¢åŠ¨({self.rsi_thresholds['æç«¯é˜ˆå€¼']['ä¸­æ³¢åŠ¨']['è¶…å–']}%,{self.rsi_thresholds['æç«¯é˜ˆå€¼']['ä¸­æ³¢åŠ¨']['è¶…ä¹°']}%), " +
                f"ä½æ³¢åŠ¨({self.rsi_thresholds['æç«¯é˜ˆå€¼']['ä½æ³¢åŠ¨']['è¶…å–']}%,{self.rsi_thresholds['æç«¯é˜ˆå€¼']['ä½æ³¢åŠ¨']['è¶…ä¹°']}%)"
            ]
        
        # ä¿å­˜åˆ°CSVï¼ˆè¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶ï¼‰
        precision = self.output_config['float_precision']
        df_copy.to_csv(filepath, encoding='utf-8-sig', float_format=f'%.{precision}f')
        
        logger.info(f"é˜ˆå€¼å·²ä¿å­˜åˆ°: {filepath}")
        return filepath
    
    def run(self, save_file: bool = True) -> pd.DataFrame:
        """
        è¿è¡Œå®Œæ•´çš„é˜ˆå€¼è®¡ç®—æµç¨‹
        
        Args:
            save_file: æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶
            
        Returns:
            è®¡ç®—ç»“æœDataFrame
        """
        try:
            # è®¡ç®—é˜ˆå€¼
            result_df = self.calculate_all_thresholds()
            
            # ä¿å­˜æ–‡ä»¶
            if save_file:
                self.save_thresholds(result_df)
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self.print_summary(result_df)
            
            return result_df
            
        except Exception as e:
            logger.error(f"è®¡ç®—è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
    
    def print_summary(self, df: pd.DataFrame):
        """æ‰“å°è®¡ç®—ç»“æœæ‘˜è¦"""
        logger.info("\n" + "="*50)
        logger.info("ç”³ä¸‡äºŒçº§è¡Œä¸šRSIé˜ˆå€¼è®¡ç®—å®Œæˆ")
        logger.info("="*50)
        logger.info(f"æ€»è¡Œä¸šæ•°é‡: {len(df)}")
        
        # æŒ‰æ³¢åŠ¨ç‡åˆ†å±‚ç»Ÿè®¡
        layer_counts = df['layer'].value_counts()
        logger.info("\næ³¢åŠ¨ç‡åˆ†å±‚ç»Ÿè®¡:")
        for layer, count in layer_counts.items():
            logger.info(f"  {layer}: {count} ä¸ªè¡Œä¸š")
        
        # é˜ˆå€¼ç»Ÿè®¡
        logger.info(f"\né˜ˆå€¼ç»Ÿè®¡:")
        logger.info(f"  æ™®é€šè¶…å–é˜ˆå€¼èŒƒå›´: {df['æ™®é€šè¶…å–'].min():.1f} - {df['æ™®é€šè¶…å–'].max():.1f}")
        logger.info(f"  æ™®é€šè¶…ä¹°é˜ˆå€¼èŒƒå›´: {df['æ™®é€šè¶…ä¹°'].min():.1f} - {df['æ™®é€šè¶…ä¹°'].max():.1f}")
        logger.info(f"  æç«¯è¶…å–é˜ˆå€¼èŒƒå›´: {df['æç«¯è¶…å–'].min():.1f} - {df['æç«¯è¶…å–'].max():.1f}")
        logger.info(f"  æç«¯è¶…ä¹°é˜ˆå€¼èŒƒå›´: {df['æç«¯è¶…ä¹°'].min():.1f} - {df['æç«¯è¶…ä¹°'].max():.1f}")
        
        logger.info("="*50)


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè®¡ç®—å™¨å®ä¾‹
    calculator = SWIndustryRSIThresholds()
    
    # è¿è¡Œè®¡ç®—
    result_df = calculator.run()
    
    # æ˜¾ç¤ºå‰å‡ è¡Œç»“æœ
    print("\nå‰10ä¸ªè¡Œä¸šçš„é˜ˆå€¼:")
    print(result_df.head(10).to_string())


if __name__ == "__main__":
    main()