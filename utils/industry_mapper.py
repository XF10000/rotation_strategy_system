"""
è‚¡ç¥¨ä»£ç åˆ°ç”³ä¸‡äºŒçº§è¡Œä¸šçš„æ˜ å°„å·¥å…·
ç”Ÿæˆå¹¶ç»´æŠ¤è‚¡ç¥¨-è¡Œä¸šæ˜ å°„ç¼“å­˜æ–‡ä»¶
"""

import akshare as ak
import pandas as pd
import json
import os
from typing import Dict, Optional
from datetime import datetime
import time

class IndustryMapper:
    """ç”³ä¸‡äºŒçº§è¡Œä¸šæ˜ å°„ç”Ÿæˆå™¨"""
    
    def __init__(self, cache_dir: str = "utils"):
        """
        åˆå§‹åŒ–æ˜ å°„ç”Ÿæˆå™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        """
        self.cache_dir = cache_dir
        self.cache_file = os.path.join(cache_dir, "stock_to_industry_map.json")
        self.retry_times = 3
        self.retry_delay = 2  # ç§’
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_shenwan_industries(self) -> pd.DataFrame:
        """
        è·å–ç”³ä¸‡äºŒçº§è¡Œä¸šåˆ—è¡¨
        
        Returns:
            pd.DataFrame: åŒ…å«è¡Œä¸šä»£ç å’Œåç§°çš„DataFrame
        """
        print("ğŸ“Š æ­£åœ¨è·å–ç”³ä¸‡äºŒçº§è¡Œä¸šåˆ—è¡¨...")
        
        for attempt in range(self.retry_times):
            try:
                # è·å–ç”³ä¸‡äºŒçº§è¡Œä¸šä¿¡æ¯
                sw_industry = ak.sw_index_second_info()
                
                if sw_industry.empty:
                    raise ValueError("AkShare APIè¿”å›ç©ºæ•°æ®")
                
                # å¤„ç†æ•°æ®æ ¼å¼
                df = sw_industry[['è¡Œä¸šä»£ç ', 'è¡Œä¸šåç§°']].copy()
                df['è¡Œä¸šä»£ç '] = df['è¡Œä¸šä»£ç '].astype(str).str.replace('.SI', '')
                df = df.rename(columns={'è¡Œä¸šä»£ç ': 'æŒ‡æ•°ä»£ç ', 'è¡Œä¸šåç§°': 'æŒ‡æ•°åç§°'})
                df = df.drop_duplicates().sort_values('æŒ‡æ•°ä»£ç ').reset_index(drop=True)
                
                print(f"âœ… æˆåŠŸè·å– {len(df)} ä¸ªç”³ä¸‡äºŒçº§è¡Œä¸š")
                return df
                
            except Exception as e:
                print(f"âš ï¸  ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < self.retry_times - 1:
                    print(f"ğŸ”„ ç­‰å¾… {self.retry_delay} ç§’åé‡è¯•...")
                    time.sleep(self.retry_delay)
                else:
                    print("âŒ è·å–ç”³ä¸‡äºŒçº§è¡Œä¸šåˆ—è¡¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®")
                    return self._get_fallback_industries()
    
    def _get_fallback_industries(self) -> pd.DataFrame:
        """
        è·å–å¤‡ç”¨çš„ç”³ä¸‡äºŒçº§è¡Œä¸šåˆ—è¡¨ï¼ˆç¡¬ç¼–ç ï¼‰
        
        Returns:
            pd.DataFrame: å¤‡ç”¨è¡Œä¸šåˆ—è¡¨
        """
        fallback_data = {
            '620100': 'æˆ¿å±‹å»ºè®¾', '620200': 'è£…ä¿®è£…é¥°', '620300': 'åŸºç¡€å»ºè®¾',
            '630100': 'é’¢é“', '630200': 'é“', '630300': 'é“œ', '630400': 'é“…é”Œ',
            '630500': 'é»„é‡‘', '630600': 'å·¥ä¸šé‡‘å±', '630700': 'è´µé‡‘å±',
            '640100': 'ç…¤ç‚­å¼€é‡‡', '640200': 'çŸ³æ²¹å¼€é‡‡', '640300': 'å¤©ç„¶æ°”',
            '650100': 'çŸ³æ²¹åŒ–å·¥', '650200': 'åŒ–å­¦åˆ¶å“', '650300': 'åŒ–å­¦çº¤ç»´',
            '650400': 'åŒ–è‚¥å†œè¯', '650500': 'æ—¥ç”¨åŒ–å·¥',
            # ... æ›´å¤šè¡Œä¸šä»£ç ï¼ˆè¿™é‡Œç®€åŒ–æ˜¾ç¤ºï¼‰
        }
        
        df = pd.DataFrame(list(fallback_data.items()), columns=['æŒ‡æ•°ä»£ç ', 'æŒ‡æ•°åç§°'])
        print(f"ğŸ“‹ ä½¿ç”¨å¤‡ç”¨æ•°æ®ï¼ŒåŒ…å« {len(df)} ä¸ªè¡Œä¸š")
        return df
    
    def get_industry_constituents(self, industry_code: str, industry_name: str) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡å®šè¡Œä¸šçš„æˆåˆ†è‚¡
        
        Args:
            industry_code: è¡Œä¸šä»£ç 
            industry_name: è¡Œä¸šåç§°
            
        Returns:
            pd.DataFrame: æˆåˆ†è‚¡ä¿¡æ¯ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        for attempt in range(self.retry_times):
            try:
                # è·å–è¡Œä¸šæˆåˆ†è‚¡
                constituents = ak.index_component_sw(symbol=industry_code)
                
                if constituents.empty:
                    print(f"âš ï¸  è¡Œä¸š {industry_name}({industry_code}) æ— æˆåˆ†è‚¡æ•°æ®")
                    return None
                
                # ç¡®ä¿åŒ…å«è‚¡ç¥¨ä»£ç åˆ—
                if 'è¯åˆ¸ä»£ç ' in constituents.columns:
                    constituents['è‚¡ç¥¨ä»£ç '] = constituents['è¯åˆ¸ä»£ç ']
                elif 'å“ç§ä»£ç ' in constituents.columns:
                    constituents['è‚¡ç¥¨ä»£ç '] = constituents['å“ç§ä»£ç ']
                elif 'ä»£ç ' in constituents.columns:
                    constituents['è‚¡ç¥¨ä»£ç '] = constituents['ä»£ç ']
                elif 'symbol' in constituents.columns:
                    constituents['è‚¡ç¥¨ä»£ç '] = constituents['symbol']
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡å‡†åˆ—åï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºè‚¡ç¥¨ä»£ç 
                    constituents['è‚¡ç¥¨ä»£ç '] = constituents.iloc[:, 0]
                
                # æ¸…ç†è‚¡ç¥¨ä»£ç æ ¼å¼
                constituents['è‚¡ç¥¨ä»£ç '] = constituents['è‚¡ç¥¨ä»£ç '].astype(str).str.strip()
                
                print(f"âœ… {industry_name}({industry_code}): {len(constituents)} åªæˆåˆ†è‚¡")
                return constituents
                
            except Exception as e:
                print(f"âš ï¸  è·å– {industry_name} æˆåˆ†è‚¡ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < self.retry_times - 1:
                    time.sleep(self.retry_delay)
                else:
                    print(f"âŒ è·å– {industry_name} æˆåˆ†è‚¡å¤±è´¥")
                    return None
    
    def generate_mapping(self) -> Dict[str, Dict[str, str]]:
        """
        ç”Ÿæˆå®Œæ•´çš„è‚¡ç¥¨-è¡Œä¸šæ˜ å°„
        
        Returns:
            Dict: è‚¡ç¥¨ä»£ç åˆ°è¡Œä¸šä¿¡æ¯çš„æ˜ å°„
        """
        print("ğŸš€ å¼€å§‹ç”Ÿæˆè‚¡ç¥¨-è¡Œä¸šæ˜ å°„...")
        
        # è·å–æ‰€æœ‰ç”³ä¸‡äºŒçº§è¡Œä¸š
        industries_df = self.get_shenwan_industries()
        
        stock_industry_map = {}
        total_industries = len(industries_df)
        processed_industries = 0
        total_stocks = 0
        
        for _, row in industries_df.iterrows():
            industry_code = row['æŒ‡æ•°ä»£ç ']
            industry_name = row['æŒ‡æ•°åç§°']
            
            processed_industries += 1
            print(f"\nğŸ“ˆ å¤„ç†è¿›åº¦: {processed_industries}/{total_industries} - {industry_name}")
            
            # è·å–è¯¥è¡Œä¸šçš„æˆåˆ†è‚¡
            constituents = self.get_industry_constituents(industry_code, industry_name)
            
            if constituents is not None:
                # å°†æˆåˆ†è‚¡æ·»åŠ åˆ°æ˜ å°„ä¸­
                for _, stock_row in constituents.iterrows():
                    stock_code = stock_row['è‚¡ç¥¨ä»£ç ']
                    
                    # è·³è¿‡æ— æ•ˆçš„è‚¡ç¥¨ä»£ç 
                    if pd.isna(stock_code) or stock_code == '' or stock_code == 'nan':
                        continue
                    
                    stock_industry_map[stock_code] = {
                        'industry_code': industry_code,
                        'industry_name': industry_name
                    }
                    total_stocks += 1
            
            # æ·»åŠ å°å»¶æ—¶ï¼Œé¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹
            time.sleep(0.5)
        
        print(f"\nğŸ‰ æ˜ å°„ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“Š æ€»è®¡å¤„ç†: {processed_industries} ä¸ªè¡Œä¸š, {total_stocks} åªè‚¡ç¥¨")
        
        return stock_industry_map
    
    def save_mapping(self, mapping: Dict[str, Dict[str, str]]) -> None:
        """
        ä¿å­˜æ˜ å°„åˆ°ç¼“å­˜æ–‡ä»¶
        
        Args:
            mapping: è‚¡ç¥¨-è¡Œä¸šæ˜ å°„å­—å…¸
        """
        try:
            # æ·»åŠ å…ƒæ•°æ®
            cache_data = {
                'metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'total_stocks': len(mapping),
                    'version': '1.0'
                },
                'mapping': mapping
            }
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ æ˜ å°„å·²ä¿å­˜åˆ°: {self.cache_file}")
            print(f"ğŸ“ æ–‡ä»¶å¤§å°: {os.path.getsize(self.cache_file) / 1024:.1f} KB")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ˜ å°„å¤±è´¥: {e}")
            raise
    
    def load_mapping(self) -> Optional[Dict[str, Dict[str, str]]]:
        """
        ä»ç¼“å­˜æ–‡ä»¶åŠ è½½æ˜ å°„
        
        Returns:
            Dict: è‚¡ç¥¨-è¡Œä¸šæ˜ å°„ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            if not os.path.exists(self.cache_file):
                print(f"âš ï¸  ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {self.cache_file}")
                return None
            
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥æ•°æ®æ ¼å¼
            if 'mapping' not in cache_data:
                print("âš ï¸  ç¼“å­˜æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘mappingå­—æ®µ")
                return None
            
            mapping = cache_data['mapping']
            metadata = cache_data.get('metadata', {})
            
            print(f"ğŸ“‚ æˆåŠŸåŠ è½½ç¼“å­˜æ˜ å°„")
            print(f"ğŸ“Š è‚¡ç¥¨æ•°é‡: {len(mapping)}")
            print(f"ğŸ• ç”Ÿæˆæ—¶é—´: {metadata.get('generated_at', 'æœªçŸ¥')}")
            
            return mapping
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def run(self, force_refresh: bool = False) -> Dict[str, Dict[str, str]]:
        """
        è¿è¡Œæ˜ å°„ç”Ÿæˆæµç¨‹
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            Dict: è‚¡ç¥¨-è¡Œä¸šæ˜ å°„
        """
        print("=" * 60)
        print("ğŸ­ ç”³ä¸‡äºŒçº§è¡Œä¸šè‚¡ç¥¨æ˜ å°„ç”Ÿæˆå™¨")
        print("=" * 60)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ–°æ˜ å°„
        if not force_refresh:
            existing_mapping = self.load_mapping()
            if existing_mapping is not None:
                print("âœ… ä½¿ç”¨ç°æœ‰ç¼“å­˜æ˜ å°„")
                return existing_mapping
        
        # ç”Ÿæˆæ–°æ˜ å°„
        print("ğŸ”„ å¼€å§‹ç”Ÿæˆæ–°çš„æ˜ å°„...")
        mapping = self.generate_mapping()
        
        # ä¿å­˜æ˜ å°„
        self.save_mapping(mapping)
        
        return mapping


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç”Ÿæˆè‚¡ç¥¨-ç”³ä¸‡äºŒçº§è¡Œä¸šæ˜ å°„ç¼“å­˜')
    parser.add_argument('--force', '-f', action='store_true', 
                       help='å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ï¼Œå³ä½¿å·²å­˜åœ¨')
    parser.add_argument('--cache-dir', default='utils',
                       help='ç¼“å­˜ç›®å½•è·¯å¾„ (é»˜è®¤: utils)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ˜ å°„ç”Ÿæˆå™¨å¹¶è¿è¡Œ
    mapper = IndustryMapper(cache_dir=args.cache_dir)
    mapping = mapper.run(force_refresh=args.force)
    
    print(f"\nğŸ¯ æ˜ å°„ç”Ÿæˆå®Œæˆï¼Œå…± {len(mapping)} åªè‚¡ç¥¨")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
    print("\nğŸ“‹ æ˜ å°„ç¤ºä¾‹:")
    for i, (stock_code, info) in enumerate(mapping.items()):
        if i >= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
            break
        print(f"  {stock_code}: {info['industry_name']}({info['industry_code']})")
    
    if len(mapping) > 5:
        print(f"  ... è¿˜æœ‰ {len(mapping) - 5} åªè‚¡ç¥¨")


if __name__ == "__main__":
    main()
