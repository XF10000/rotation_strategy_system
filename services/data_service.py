"""
æ•°æ®æœåŠ¡
è´Ÿè´£æ‰€æœ‰æ•°æ®è·å–ã€ç¼“å­˜ã€å¤„ç†å’ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—
"""

from datetime import datetime, timedelta
from typing import Any, Dict, Optional

import pandas as pd

from config.path_manager import get_path_manager
from data.data_fetcher import DataFetcherFactory
from data.data_processor import DataProcessor
from data.data_storage import DataStorage
from pipelines import DataPipeline, DataValidator, DataNormalizer

from .base_service import BaseService


class DataService(BaseService):
    """
    æ•°æ®æœåŠ¡ - ç»Ÿä¸€çš„æ•°æ®è®¿é—®å±‚
    
    èŒè´£ï¼š
    1. è‚¡ç¥¨æ•°æ®è·å–ï¼ˆç½‘ç»œ/ç¼“å­˜ï¼‰
    2. æ•°æ®ç¼“å­˜ç®¡ç†
    3. æ•°æ®é¢„å¤„ç†
    4. æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
    5. DCFä¼°å€¼æ•°æ®åŠ è½½
    6. RSIé˜ˆå€¼æ•°æ®åŠ è½½
    7. è‚¡ç¥¨-è¡Œä¸šæ˜ å°„åŠ è½½
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ•°æ®æœåŠ¡
        
        Args:
            config: é…ç½®å­—å…¸
        """
        super().__init__(config)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_fetcher = None
        self.data_processor = DataProcessor()
        self.data_storage = DataStorage()
        
        # åˆ›å»ºæ•°æ®å¤„ç†ç®¡é“
        self.data_pipeline = (DataPipeline()
            .add_step(DataValidator())
            .add_step(DataNormalizer(fill_method='ffill', remove_duplicates=True))
        )
        self.logger.info(f"ğŸ“Š æ•°æ®ç®¡é“å·²åˆ›å»º: {self.data_pipeline.get_steps()}")
        
        # æ•°æ®ç¼“å­˜
        self.stock_data: Dict[str, Dict[str, pd.DataFrame]] = {}
        self.dcf_values: Dict[str, float] = {}
        self.rsi_thresholds: Dict[str, Dict[str, float]] = {}
        self.stock_industry_map: Dict[str, Dict[str, str]] = {}
        
        # é…ç½®å‚æ•°
        self.start_date = config.get('start_date', '2022-01-01')
        self.end_date = config.get('end_date', '2024-12-31')
        
        # ä»initial_holdingsæå–è‚¡ç¥¨æ± ï¼ˆæ’é™¤ç°é‡‘ï¼‰
        initial_holdings = config.get('initial_holdings', {})
        self.stock_pool = [code for code in initial_holdings.keys() if code.lower() != 'cash']
        
        # å¦‚æœstock_poolä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–é…ç½®ä¸­è·å–
        if not self.stock_pool:
            self.logger.warning("ä»initial_holdingsä¸­æœªæ‰¾åˆ°è‚¡ç¥¨ï¼Œå°è¯•å…¶ä»–é…ç½®æº...")
            # å¯ä»¥ä»å…¶ä»–åœ°æ–¹è·å–è‚¡ç¥¨æ± 
    
    def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æ•°æ®æœåŠ¡
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            # åˆ›å»ºæ•°æ®è·å–å™¨
            data_source = self.config.get('data_source', 'akshare')
            self.data_fetcher = DataFetcherFactory.create_fetcher(data_source, self.config)
            
            # åŠ è½½é…ç½®æ•°æ®
            self.dcf_values = self.load_dcf_values()
            self.rsi_thresholds = self.load_rsi_thresholds()
            self.stock_industry_map = self.load_stock_industry_map()
            
            self._initialized = True
            self.logger.info("DataService åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            self.logger.error(f"DataService åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def prepare_backtest_data(self) -> bool:
        """
        å‡†å¤‡å›æµ‹æ•°æ®ï¼ˆæ™ºèƒ½ç¼“å­˜ç‰ˆæœ¬ï¼‰
        
        Returns:
            bool: æ•°æ®å‡†å¤‡æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹å‡†å¤‡å›æµ‹æ•°æ®ï¼ˆæ™ºèƒ½ç¼“å­˜æ¨¡å¼ï¼‰...")
            
            # è®¡ç®—æ‰©å±•çš„å¼€å§‹æ—¥æœŸ
            start_date_obj = datetime.strptime(self.start_date, '%Y-%m-%d')
            total_history_weeks = 125 + 14  # 125å‘¨æŠ€æœ¯æŒ‡æ ‡ + 14å‘¨RSIé¢„çƒ­
            extended_start_date = start_date_obj - timedelta(weeks=total_history_weeks)
            extended_start_date_str = extended_start_date.strftime('%Y-%m-%d')
            
            self.logger.info(f"ğŸ“… å›æµ‹æœŸé—´: {self.start_date} è‡³ {self.end_date}")
            self.logger.info(f"ğŸ“… æ•°æ®è·å–æœŸé—´ï¼ˆå«139å‘¨å†å²ç¼“å†²ï¼‰: {extended_start_date_str} è‡³ {self.end_date}")
            
            # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
            cache_stats = self.data_storage.get_cache_statistics()
            self.logger.info(f"ğŸ“Š å½“å‰ç¼“å­˜ç»Ÿè®¡: {cache_stats}")
            
            for stock_code in self.stock_pool:
                self.logger.info(f"ğŸ“ˆ å‡†å¤‡ {stock_code} çš„å†å²æ•°æ®...")
                
                # 1. è·å–æ—¥çº¿æ•°æ®
                daily_data = self._get_cached_or_fetch_data(
                    stock_code, extended_start_date_str, self.end_date, 'daily'
                )
                
                if daily_data is None or daily_data.empty:
                    self.logger.warning(f"âš ï¸ {stock_code} åˆæ¬¡è·å–æ•°æ®ä¸ºç©ºï¼Œå°è¯•æ™ºèƒ½æ‰©å±•æ—¥æœŸèŒƒå›´")
                    daily_data = self._get_data_with_smart_expansion(
                        stock_code, extended_start_date_str, self.end_date, 'daily'
                    )
                    
                    if daily_data is None or daily_data.empty:
                        self.logger.warning(f"âš ï¸ {stock_code} æ‰©å±•è·å–åä»æ— æ•°æ®ï¼Œè·³è¿‡è¯¥è‚¡ç¥¨")
                        continue
                    else:
                        self.logger.info(f"âœ… {stock_code} é€šè¿‡æ™ºèƒ½æ‰©å±•æˆåŠŸè·å–åˆ° {len(daily_data)} æ¡æ•°æ®")
                
                # 2. è·å–æˆ–ç”Ÿæˆå‘¨çº¿æ•°æ®
                weekly_data = self._get_or_generate_weekly_data(
                    stock_code, daily_data, extended_start_date_str
                )
                
                if weekly_data is None or weekly_data.empty:
                    self.logger.warning(f"âš ï¸ {stock_code} å‘¨çº¿æ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡è¯¥è‚¡ç¥¨")
                    continue
                
                # 3. è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                weekly_data = self._ensure_technical_indicators(stock_code, weekly_data)
                
                # 4. è·å–åˆ†çº¢é…è‚¡æ•°æ®
                weekly_data = self._process_dividend_data(
                    stock_code, weekly_data, extended_start_date_str
                )
                
                # 5. å­˜å‚¨æ•°æ®
                self.stock_data[stock_code] = {
                    'daily': daily_data,
                    'weekly': weekly_data
                }
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                actual_start_date = pd.to_datetime(self.start_date)
                weekly_backtest_data = weekly_data[weekly_data.index >= actual_start_date]
                
                self.logger.info(f"âœ… {stock_code} æ•°æ®å‡†å¤‡å®Œæˆ:")
                self.logger.info(f"   - æ—¥çº¿æ•°æ®: {len(daily_data)} æ¡")
                self.logger.info(f"   - å‘¨çº¿æ•°æ®: {len(weekly_data)} æ¡ (å›æµ‹æœŸ {len(weekly_backtest_data)} æ¡)")
                
                # RSIç»Ÿè®¡
                rsi_valid = weekly_backtest_data['rsi'].notna().sum()
                rsi_nan = weekly_backtest_data['rsi'].isna().sum()
                self.logger.info(f"   - RSI: {rsi_valid} æœ‰æ•ˆå€¼, {rsi_nan} NaNå€¼")
            
            self.logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼Œå…± {len(self.stock_data)} åªè‚¡ç¥¨")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False
    
    def get_stock_data(self, stock_code: str, freq: str = 'weekly') -> Optional[pd.DataFrame]:
        """
        è·å–è‚¡ç¥¨æ•°æ®
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            freq: é¢‘ç‡ ('daily' æˆ– 'weekly')
            
        Returns:
            è‚¡ç¥¨æ•°æ®DataFrameï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        if stock_code not in self.stock_data:
            return None
        return self.stock_data[stock_code].get(freq)
    
    def get_all_stock_data(self, freq: str = 'weekly') -> Dict[str, pd.DataFrame]:
        """
        è·å–æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®
        
        Args:
            freq: é¢‘ç‡ ('daily' æˆ– 'weekly')
            
        Returns:
            è‚¡ç¥¨ä»£ç åˆ°æ•°æ®çš„æ˜ å°„
        """
        return {code: data[freq] for code, data in self.stock_data.items() if freq in data}
    
    def load_dcf_values(self) -> Dict[str, float]:
        """
        ä»CSVé…ç½®æ–‡ä»¶åŠ è½½DCFä¼°å€¼æ•°æ®
        
        Returns:
            è‚¡ç¥¨ä»£ç åˆ°DCFä¼°å€¼çš„æ˜ å°„
        """
        try:
            path_manager = get_path_manager()
            portfolio_config_path = path_manager.get_portfolio_config_path()
            
            df = pd.read_csv(portfolio_config_path, encoding='utf-8-sig')
            dcf_values = {}
            
            for _, row in df.iterrows():
                stock_code = row['Stock_number']
                if stock_code != 'CASH':
                    dcf_value = row.get('DCF_value_per_share', None)
                    if dcf_value is not None and pd.notna(dcf_value):
                        dcf_values[stock_code] = float(dcf_value)
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(dcf_values)} åªè‚¡ç¥¨çš„DCFä¼°å€¼")
            return dcf_values
        except Exception as e:
            self.logger.warning(f"DCFä¼°å€¼æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def load_rsi_thresholds(self) -> Dict[str, Dict[str, float]]:
        """
        ä»CSVæ–‡ä»¶åŠ è½½ç”³ä¸‡äºŒçº§è¡Œä¸šRSIé˜ˆå€¼æ•°æ®
        
        Returns:
            è¡Œä¸šä»£ç åˆ°RSIé˜ˆå€¼çš„æ˜ å°„
        """
        try:
            import os
            
            rsi_file_path = 'sw_rsi_thresholds/output/sw2_rsi_threshold.csv'
            
            if not os.path.exists(rsi_file_path):
                self.logger.warning(f"RSIé˜ˆå€¼æ–‡ä»¶ä¸å­˜åœ¨: {rsi_file_path}")
                return {}
            
            df = pd.read_csv(rsi_file_path, encoding='utf-8-sig')
            rsi_thresholds = {}
            
            for _, row in df.iterrows():
                industry_code = str(row['è¡Œä¸šä»£ç ']).strip()
                rsi_thresholds[industry_code] = {
                    'industry_name': row.get('è¡Œä¸šåç§°', ''),
                    'buy_threshold': float(row.get('æ™®é€šè¶…å–', 30)),
                    'sell_threshold': float(row.get('æ™®é€šè¶…ä¹°', 70)),
                    'extreme_buy_threshold': float(row.get('æç«¯è¶…å–', 20)),
                    'extreme_sell_threshold': float(row.get('æç«¯è¶…ä¹°', 80)),
                    'volatility_level': row.get('layer', 'medium'),
                    'volatility': float(row.get('volatility', 0)),
                    'current_rsi': float(row.get('current_rsi', 50))
                }
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(rsi_thresholds)} ä¸ªè¡Œä¸šçš„RSIé˜ˆå€¼")
            return rsi_thresholds
            
        except Exception as e:
            self.logger.warning(f"RSIé˜ˆå€¼æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def load_stock_industry_map(self) -> Dict[str, Dict[str, str]]:
        """
        ä»JSONæ–‡ä»¶åŠ è½½è‚¡ç¥¨-è¡Œä¸šæ˜ å°„æ•°æ®
        
        Returns:
            è‚¡ç¥¨ä»£ç åˆ°è¡Œä¸šä¿¡æ¯çš„æ˜ å°„
        """
        try:
            import json
            import os
            
            map_file_path = 'utils/stock_to_industry_map.json'
            
            if not os.path.exists(map_file_path):
                self.logger.warning(f"è‚¡ç¥¨-è¡Œä¸šæ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {map_file_path}")
                return {}
            
            with open(map_file_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æå–mappingå­—æ®µ
            if 'mapping' not in cache_data:
                self.logger.warning("æ˜ å°„æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘mappingå­—æ®µ")
                return {}
            
            stock_industry_map = cache_data['mapping']
            metadata = cache_data.get('metadata', {})
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(stock_industry_map)} åªè‚¡ç¥¨çš„è¡Œä¸šæ˜ å°„")
            self.logger.info(f"ğŸ“Š æ˜ å°„æ•°æ®ç‰ˆæœ¬: {metadata.get('version', 'æœªçŸ¥')}")
            self.logger.info(f"ğŸ• ç”Ÿæˆæ—¶é—´: {metadata.get('generated_at', 'æœªçŸ¥')}")
            
            return stock_industry_map
            
        except Exception as e:
            self.logger.warning(f"è‚¡ç¥¨-è¡Œä¸šæ˜ å°„åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def _get_cached_or_fetch_data(self, stock_code: str, start_date: str, 
                                   end_date: str, freq: str) -> Optional[pd.DataFrame]:
        """
        æ™ºèƒ½è·å–æ•°æ®ï¼ˆä¼˜å…ˆç¼“å­˜ï¼Œç¼“å­˜å¤±è´¥åˆ™ç½‘ç»œè·å–ï¼‰
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            freq: é¢‘ç‡
            
        Returns:
            è‚¡ç¥¨æ•°æ®DataFrame
        """
        try:
            # 1. å°è¯•ä»ç¼“å­˜åŠ è½½
            cached_data = self.data_storage.load_data(stock_code, freq)
            
            if cached_data is not None and not cached_data.empty:
                # æ£€æŸ¥ç¼“å­˜æ•°æ®æ˜¯å¦è¦†ç›–æ‰€éœ€æ—¥æœŸèŒƒå›´
                cache_start = cached_data.index.min()
                cache_end = cached_data.index.max()
                required_start = pd.to_datetime(start_date)
                required_end = pd.to_datetime(end_date)
                
                if cache_start <= required_start and cache_end >= required_end:
                    self.logger.info(f"âœ… {stock_code} ä»ç¼“å­˜åŠ è½½{freq}æ•°æ®")
                    return cached_data[(cached_data.index >= required_start) & 
                                      (cached_data.index <= required_end)]
            
            # 2. ç¼“å­˜ä¸å¯ç”¨ï¼Œä»ç½‘ç»œè·å–
            self.logger.info(f"ğŸŒ {stock_code} ä»ç½‘ç»œè·å–{freq}æ•°æ®")
            data = self.data_fetcher.get_stock_data(stock_code, start_date, end_date, freq)
            
            if data is not None and not data.empty:
                # ä¿å­˜åˆ°ç¼“å­˜
                self.data_storage.save_data(data, stock_code, freq)
                self.logger.info(f"ğŸ’¾ {stock_code} {freq}æ•°æ®å·²ä¿å­˜åˆ°ç¼“å­˜")
                return data
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ {stock_code} æ•°æ®è·å–å¤±è´¥: {e}")
            return None
    
    def _get_data_with_smart_expansion(self, stock_code: str, start_date: str,
                                       end_date: str, freq: str) -> Optional[pd.DataFrame]:
        """
        æ™ºèƒ½æ‰©å±•æ—¥æœŸèŒƒå›´è·å–æ•°æ®
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            freq: é¢‘ç‡
            
        Returns:
            è‚¡ç¥¨æ•°æ®DataFrame
        """
        try:
            # å‘å‰æ‰©å±•30å¤©
            extended_start = (pd.to_datetime(start_date) - pd.Timedelta(days=30)).strftime('%Y-%m-%d')
            # å‘åæ‰©å±•30å¤©
            extended_end = (pd.to_datetime(end_date) + pd.Timedelta(days=30)).strftime('%Y-%m-%d')
            
            self.logger.info(f"ğŸ”„ {stock_code} æ‰©å±•æ—¥æœŸèŒƒå›´: {extended_start} è‡³ {extended_end}")
            
            data = self.data_fetcher.get_stock_data(stock_code, extended_start, extended_end, freq)
            
            if data is not None and not data.empty:
                # è£å‰ªå›åŸå§‹æ—¥æœŸèŒƒå›´
                original_start = pd.to_datetime(start_date)
                original_end = pd.to_datetime(end_date)
                data = data[(data.index >= original_start) & (data.index <= original_end)]
                
                # ä¿å­˜åˆ°ç¼“å­˜
                if not data.empty:
                    self.data_storage.save_data(data, stock_code, freq)
                
                return data
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ {stock_code} æ™ºèƒ½æ‰©å±•è·å–å¤±è´¥: {e}")
            return None
    
    def _get_or_generate_weekly_data(self, stock_code: str, daily_data: pd.DataFrame,
                                     extended_start_date: str) -> Optional[pd.DataFrame]:
        """
        è·å–æˆ–ç”Ÿæˆå‘¨çº¿æ•°æ®
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            daily_data: æ—¥çº¿æ•°æ®
            extended_start_date: æ‰©å±•å¼€å§‹æ—¥æœŸ
            
        Returns:
            å‘¨çº¿æ•°æ®DataFrame
        """
        try:
            # å§‹ç»ˆä»æ—¥çº¿æ•°æ®è½¬æ¢å‘¨çº¿ï¼Œç¡®ä¿end_dateå‚æ•°è¢«æ­£ç¡®ä¼ é€’
            # ä¸ä½¿ç”¨ç¼“å­˜çš„å‘¨çº¿æ•°æ®ï¼Œå› ä¸ºç¼“å­˜å¯èƒ½ä¸åŒ…å«æ­£ç¡®çš„end_dateå¤„ç†
            self.logger.info(f"ğŸ”„ {stock_code} ä»æ—¥çº¿æ•°æ®è½¬æ¢å‘¨çº¿æ•°æ®")
            weekly_data = self.data_processor.resample_to_weekly(daily_data, end_date=self.end_date)
            
            if len(weekly_data) < 139:
                self.logger.warning(f"âš ï¸ {stock_code} æ•°æ®ä¸è¶³ï¼Œåªæœ‰ {len(weekly_data)} æ¡è®°å½•ï¼Œå»ºè®®139æ¡")
            
            return weekly_data
            
        except Exception as e:
            self.logger.error(f"âŒ {stock_code} å‘¨çº¿æ•°æ®è·å–å¤±è´¥: {e}")
            return None
    
    def _ensure_technical_indicators(self, stock_code: str, 
                                     weekly_data: pd.DataFrame) -> pd.DataFrame:
        """
        ç¡®ä¿æŠ€æœ¯æŒ‡æ ‡å­˜åœ¨ä¸”æœ‰æ•ˆ
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            weekly_data: å‘¨çº¿æ•°æ®
            
        Returns:
            åŒ…å«æŠ€æœ¯æŒ‡æ ‡çš„å‘¨çº¿æ•°æ®
        """
        try:
            need_recalculate = False
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è®¡ç®—
            if 'ema_20' not in weekly_data.columns or 'rsi' not in weekly_data.columns:
                need_recalculate = True
                self.logger.info(f"ğŸ”§ {stock_code} æŠ€æœ¯æŒ‡æ ‡åˆ—ä¸å­˜åœ¨ï¼Œéœ€è¦è®¡ç®—")
            else:
                # æ£€æŸ¥æœ€æ–°å‡ è¡Œæ˜¯å¦æœ‰NaN
                recent_data = weekly_data.tail(5)
                rsi_nan_count = recent_data['rsi'].isna().sum()
                macd_nan_count = recent_data['macd'].isna().sum()
                
                if rsi_nan_count > 0 or macd_nan_count > 0:
                    need_recalculate = True
                    self.logger.info(f"ğŸ”§ {stock_code} æœ€æ–°æŠ€æœ¯æŒ‡æ ‡æœ‰NaNå€¼ï¼Œéœ€è¦é‡æ–°è®¡ç®—")
            
            if need_recalculate:
                # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                if len(weekly_data) < 30:
                    self.logger.warning(f"âš ï¸ {stock_code} æ•°æ®é‡ä¸è¶³ ({len(weekly_data)} < 30)")
                
                self.logger.info(f"ğŸ”§ {stock_code} å¼€å§‹è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼Œæ•°æ®é‡: {len(weekly_data)}")
                
                # ä½¿ç”¨æ•°æ®ç®¡é“å¤„ç†æ•°æ®ï¼ˆéªŒè¯å’Œæ ‡å‡†åŒ–ï¼‰
                try:
                    weekly_data = self.data_pipeline.process(weekly_data)
                    self.logger.debug(f"âœ… {stock_code} æ•°æ®ç®¡é“å¤„ç†å®Œæˆ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {stock_code} æ•°æ®ç®¡é“å¤„ç†å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
                
                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                weekly_data = self.data_processor.calculate_technical_indicators(weekly_data)
                self.logger.info(f"âœ… {stock_code} æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆ")
                
                # ä¿å­˜åˆ°ç¼“å­˜
                try:
                    self.data_storage.save_data(weekly_data, stock_code, 'weekly')
                    self.logger.info(f"ğŸ’¾ {stock_code} å‘¨çº¿æ•°æ®ï¼ˆå«æŠ€æœ¯æŒ‡æ ‡ï¼‰å·²ä¿å­˜åˆ°ç¼“å­˜")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ {stock_code} å‘¨çº¿æ•°æ®ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            else:
                self.logger.info(f"âœ… {stock_code} æŠ€æœ¯æŒ‡æ ‡å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè·³è¿‡è®¡ç®—")
            
            return weekly_data
            
        except Exception as e:
            self.logger.error(f"âŒ {stock_code} æŠ€æœ¯æŒ‡æ ‡å¤„ç†å¤±è´¥: {e}")
            return weekly_data
    
    def _process_dividend_data(self, stock_code: str, weekly_data: pd.DataFrame,
                               extended_start_date: str) -> pd.DataFrame:
        """
        å¤„ç†åˆ†çº¢é…è‚¡æ•°æ®
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            weekly_data: å‘¨çº¿æ•°æ®
            extended_start_date: æ‰©å±•å¼€å§‹æ—¥æœŸ
            
        Returns:
            åŒ…å«åˆ†çº¢ä¿¡æ¯çš„å‘¨çº¿æ•°æ®
        """
        try:
            self.logger.info(f"ğŸ’° {stock_code} è·å–åˆ†çº¢é…è‚¡æ•°æ®...")
            dividend_data = self.data_fetcher.get_dividend_data(
                stock_code, extended_start_date, self.end_date
            )
            
            if not dividend_data.empty:
                self.logger.info(f"âœ… {stock_code} è·å–åˆ° {len(dividend_data)} æ¡åˆ†çº¢è®°å½•")
                weekly_data = self.data_fetcher.align_dividend_with_weekly_data(
                    weekly_data, dividend_data
                )
                self.logger.info(f"âœ… {stock_code} åˆ†çº¢æ•°æ®å·²å¯¹é½åˆ°å‘¨çº¿æ•°æ®")
                
                # æ£€æŸ¥å¯¹é½åçš„åˆ†çº¢äº‹ä»¶
                dividend_weeks = weekly_data[weekly_data['dividend_amount'] > 0]
                if not dividend_weeks.empty:
                    self.logger.info(f"ğŸ’° {stock_code} å¯¹é½åˆ° {len(dividend_weeks)} ä¸ªåˆ†çº¢äº‹ä»¶")
                    for date, row in dividend_weeks.iterrows():
                        self.logger.info(f"  {date.strftime('%Y-%m-%d')}: æ´¾æ¯ {row['dividend_amount']}å…ƒ")
            else:
                self.logger.info(f"âš ï¸ {stock_code} æœªè·å–åˆ°åˆ†çº¢æ•°æ®")
            
            return weekly_data
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ {stock_code} åˆ†çº¢æ•°æ®è·å–å¤±è´¥: {e}")
            return weekly_data
    
    def get_dcf_value(self, stock_code: str) -> Optional[float]:
        """è·å–è‚¡ç¥¨çš„DCFä¼°å€¼"""
        return self.dcf_values.get(stock_code)
    
    def get_rsi_thresholds(self, industry_code: str) -> Optional[Dict[str, float]]:
        """è·å–è¡Œä¸šçš„RSIé˜ˆå€¼"""
        return self.rsi_thresholds.get(industry_code)
    
    def get_stock_industry(self, stock_code: str) -> Optional[Dict[str, str]]:
        """è·å–è‚¡ç¥¨çš„è¡Œä¸šä¿¡æ¯"""
        return self.stock_industry_map.get(stock_code)
